# a30_020_make_vector_store_id.py
# python a30_020_make_vector_store_id.py
# 4ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’OpenAI Vector Storeã«ç™»éŒ²ã™ã‚‹å®Œå…¨ç‰ˆ
# OUTPUT/customer_support_faq.txt
# OUTPUT/medical_qa.txt
# OUTPUT/sciq_qa.txt
# OUTPUT/legal_qa.txt

import os
import re
import time
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import logging

from openai import OpenAI
from openai.types.chat import ChatCompletionMessageParam, ChatCompletionSystemMessageParam, \
    ChatCompletionUserMessageParam, ChatCompletionAssistantMessageParam

import pandas as pd
import tempfile
import textwrap
from pydantic import BaseModel, Field
from tqdm import tqdm

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from helper_st import (
        UIHelper, MessageManagerUI, ResponseProcessorUI,
        SessionStateManager, error_handler_ui, timer_ui,
        init_page, select_model, InfoPanelManager
    )
    from helper_api import (
        config, logger, TokenManager, OpenAIClient,
        EasyInputMessageParam, ResponseInputTextParam,
        ConfigManager, MessageManager, sanitize_key,
        error_handler, timer
    )
    from helper_rag import (
        RAGConfig, clean_text, combine_columns,
        additional_preprocessing, validate_data
    )
except ImportError as e:
    # streamlitãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    logger.warning(f"ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")

BASE_DIR = Path(__file__).resolve().parent.parent  # Paslib
THIS_DIR = Path(__file__).resolve().parent  # Paslib
OUTPUT_DIR = THIS_DIR / "OUTPUT"

# ==================================================
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
# ==================================================
DATASET_CONFIGS = {
    "customer_support_faq": {
        "filename"   : "customer_support_faq.txt",
        "store_name" : "Customer Support FAQ Knowledge Base",
        "description": "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹",
        "chunk_size" : 1000,
        "overlap"    : 100
    },
    "medical_qa"          : {
        "filename"   : "medical_qa.txt",
        "store_name" : "Medical Q&A Knowledge Base",
        "description": "åŒ»ç™‚è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹",
        "chunk_size" : 1500,
        "overlap"    : 150
    },
    "sciq_qa"             : {
        "filename"   : "sciq_qa.txt",
        "store_name" : "Science & Technology Q&A Knowledge Base",
        "description": "ç§‘å­¦æŠ€è¡“è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹",
        "chunk_size" : 800,
        "overlap"    : 80
    },
    "legal_qa"            : {
        "filename"   : "legal_qa.txt",
        "store_name" : "Legal Q&A Knowledge Base",
        "description": "æ³•å¾‹è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹",
        "chunk_size" : 1200,
        "overlap"    : 120
    }
}


# ==================================================
# ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–¢æ•°
# ==================================================
def load_text_file(filepath: Path) -> List[str]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€è¡Œã”ã¨ã®ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã™

    Args:
        filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        List[str]: ãƒ†ã‚­ã‚¹ãƒˆè¡Œã®ãƒªã‚¹ãƒˆ
    """
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # ç©ºè¡Œã¨çŸ­ã™ãã‚‹è¡Œã‚’é™¤å»
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if line and len(line) > 10:  # 10æ–‡å­—ä»¥ä¸Šã®è¡Œã®ã¿ä¿æŒ
                cleaned_lines.append(line)

        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {filepath.name} - {len(cleaned_lines)}è¡Œ")
        return cleaned_lines

    except FileNotFoundError:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
        return []
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {filepath} - {e}")
        return []


def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:
    """
    é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šã‚µã‚¤ã‚ºã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²

    Args:
        text: åˆ†å‰²å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
        chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—æ•°ï¼‰
        overlap: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—æ•°ï¼‰

    Returns:
        List[str]: åˆ†å‰²ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯
    """
    if len(text) <= chunk_size:
        return [text]

    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size

        # æ–‡ã®å¢ƒç•Œã§åˆ†å‰²ã™ã‚‹ã‚ˆã†ã«èª¿æ•´
        if end < len(text):
            # å¥èª­ç‚¹ã‚’æ¢ã™
            for punct in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']:
                punct_pos = text.rfind(punct, start, end)
                if punct_pos > start + chunk_size // 2:
                    end = punct_pos + 1
                    break

        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)

        # æ¬¡ã®é–‹å§‹ä½ç½®ã‚’è¨­å®šï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’è€ƒæ…®ï¼‰
        start = max(start + 1, end - overlap)

        # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
        if start >= len(text):
            break

    return chunks


def text_to_jsonl_data(lines: List[str], dataset_type: str) -> List[Dict[str, str]]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆè¡Œã‚’JSONLç”¨ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¤‰æ›

    Args:
        lines: ãƒ†ã‚­ã‚¹ãƒˆè¡Œã®ãƒªã‚¹ãƒˆ
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—

    Returns:
        List[Dict]: JSONLç”¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ãƒªã‚¹ãƒˆ
    """
    config = DATASET_CONFIGS.get(dataset_type, {})
    chunk_size = config.get('chunk_size', 1000)
    overlap = config.get('overlap', 100)

    jsonl_data = []

    for idx, line in enumerate(lines):
        # ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        cleaned_text = clean_text(line)

        if not cleaned_text:
            continue

        # é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        chunks = chunk_text(cleaned_text, chunk_size, overlap)

        for chunk_idx, chunk in enumerate(chunks):
            jsonl_entry = {
                "id"      : f"{dataset_type}_{idx}_{chunk_idx}",
                "text"    : chunk,
                "metadata": {
                    "dataset"      : dataset_type,
                    "original_line": idx,
                    "chunk_index"  : chunk_idx,
                    "total_chunks" : len(chunks)
                }
            }
            jsonl_data.append(jsonl_entry)

    logger.info(f"{dataset_type}: {len(lines)}è¡Œ -> {len(jsonl_data)}ãƒãƒ£ãƒ³ã‚¯")
    return jsonl_data


# ==================================================
# æ—¢å­˜ã®é–¢æ•°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
# ==================================================
def get_embeddings_batch(texts: List[str], model: str = "text-embedding-3-small", batch_size: int = 100) -> List[
    Optional[List[float]]]:
    """
    OpenAI Embedding APIã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã‚’ãƒãƒƒãƒã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–

    Args:
        texts: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        model: ä½¿ç”¨ã™ã‚‹embeddingãƒ¢ãƒ‡ãƒ« (æ¨å¥¨: text-embedding-3-small)
        batch_size: ä¸€åº¦ã®APIå‘¼ã³å‡ºã—ã§å‡¦ç†ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆæ•°ï¼ˆæœ€å¤§2048ï¼‰

    Returns:
        åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆ
    """
    client = OpenAI()
    embeddings = []

    # ãƒ†ã‚­ã‚¹ãƒˆã‚’å‰å‡¦ç†ï¼ˆæ”¹è¡Œæ–‡å­—ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›ï¼‰
    cleaned_texts = [text.replace("\n", " ") for text in texts]

    logger.info(f"Embeddingä½œæˆé–‹å§‹: {len(cleaned_texts)}ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’{batch_size}ä»¶ãšã¤å‡¦ç†")

    # ãƒãƒƒãƒã”ã¨ã«å‡¦ç†
    for i in tqdm(range(0, len(cleaned_texts), batch_size), desc="Embeddingä½œæˆä¸­"):
        batch = cleaned_texts[i:i + batch_size]

        try:
            response = client.embeddings.create(
                input=batch,
                model=model,
                # dimensions=1024  # ã‚³ã‚¹ãƒˆåŠ¹ç‡ã‚’é‡è¦–ã™ã‚‹å ´åˆã¯æ¬¡å…ƒæ•°ã‚’å‰Šæ¸›
            )

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰embeddingã‚’å–å¾—
            batch_embeddings = [data.embedding for data in response.data]
            embeddings.extend(batch_embeddings)

            logger.info(
                f"ãƒãƒƒãƒ {i // batch_size + 1}/{(len(cleaned_texts) - 1) // batch_size + 1}: {len(batch)}ä»¶å‡¦ç†å®Œäº†")

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œã®ãŸã‚ã®çŸ­ã„å¾…æ©Ÿ
            time.sleep(0.1)

        except Exception as e:
            logger.error(f"ãƒãƒƒãƒ {i // batch_size + 1}ã§ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯Noneã§åŸ‹ã‚ã‚‹
            embeddings.extend([None] * len(batch))

            # ä¸€æ™‚çš„ãªå•é¡Œã®å ´åˆã¯ãƒªãƒˆãƒ©ã‚¤
            time.sleep(2)

    return embeddings


def create_vector_store_from_jsonl_data(jsonl_data: List[Dict], store_name: str) -> Optional[str]:
    """
    JSONLå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Vector Storeã‚’ä½œæˆ

    Args:
        jsonl_data: JSONLå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ
        store_name: Vector Storeã®åå‰

    Returns:
        Vector Store IDï¼ˆæˆåŠŸæ™‚ï¼‰ã¾ãŸã¯Noneï¼ˆå¤±æ•—æ™‚ï¼‰
    """
    client = OpenAI()
    temp_file_path = None
    uploaded_file_id = None

    try:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as temp_file:
            for entry in jsonl_data:
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯æ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜ï¼ˆOpenAIå´ã®åˆ¶é™å¯¾å¿œï¼‰
                jsonl_entry = {
                    "id"  : entry["id"],
                    "text": entry["text"]
                }
                temp_file.write(json.dumps(jsonl_entry, ensure_ascii=False) + '\n')

            temp_file_path = temp_file.name

        logger.info(f"JSONLãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {temp_file_path} ({len(jsonl_data)}ã‚¨ãƒ³ãƒˆãƒª)")

        # Step 1: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’OpenAIã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        with open(temp_file_path, 'rb') as file:
            uploaded_file = client.files.create(
                file=file,
                purpose="assistants"
            )
            uploaded_file_id = uploaded_file.id

        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: File ID={uploaded_file_id}")

        # Step 2: Vector Storeã‚’ä½œæˆ
        vector_store = client.vector_stores.create(
            name=store_name,
            metadata={
                "created_by" : "vector_store_creator",
                "version"    : "2025.1",
                "data_format": "jsonl_as_txt",
                "entry_count": str(len(jsonl_data))
            }
        )

        logger.info(f"Vector Storeä½œæˆå®Œäº†: ID={vector_store.id}")

        # Step 3: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Vector Storeã«è¿½åŠ 
        vector_store_file = client.vector_stores.files.create(
            vector_store_id=vector_store.id,
            file_id=uploaded_file_id
        )

        logger.info(f"Vector StoreFileãƒªãƒ³ã‚¯ä½œæˆ: {vector_store_file.id}")

        # Step 4: ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†ã‚’å¾…æ©Ÿ
        max_wait_time = 600  # æœ€å¤§10åˆ†å¾…æ©Ÿ
        wait_interval = 5  # 5ç§’é–“éš”ã§ãƒã‚§ãƒƒã‚¯
        waited_time = 0

        while waited_time < max_wait_time:
            file_status = client.vector_stores.files.retrieve(
                vector_store_id=vector_store.id,
                file_id=uploaded_file_id
            )

            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†çŠ¶æ³: {file_status.status} (å¾…æ©Ÿæ™‚é–“: {waited_time}ç§’)")

            if file_status.status == "completed":
                updated_vector_store = client.vector_stores.retrieve(vector_store.id)

                logger.info(f"âœ… Vector Storeä½œæˆå®Œäº†:")
                logger.info(f"  - ID: {vector_store.id}")
                logger.info(f"  - Name: {vector_store.name}")
                logger.info(f"  - ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†çŠ¶æ³: {file_status.status}")
                logger.info(f"  - ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {updated_vector_store.file_counts.total}")
                logger.info(f"  - å®Œäº†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {updated_vector_store.file_counts.completed}")
                logger.info(f"  - å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {updated_vector_store.file_counts.failed}")
                logger.info(f"  - ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡: {updated_vector_store.usage_bytes} bytes")

                return vector_store.id

            elif file_status.status == "failed":
                logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¤±æ•—: {file_status.last_error}")
                return None

            elif file_status.status in ["in_progress", "cancelling"]:
                time.sleep(wait_interval)
                waited_time += wait_interval
            else:
                logger.warning(f"âš ï¸ äºˆæœŸã—ãªã„ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {file_status.status}")
                time.sleep(wait_interval)
                waited_time += wait_interval

        logger.error(f"âŒ Vector Storeä½œæˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (åˆ¶é™æ™‚é–“: {max_wait_time}ç§’)")
        return None

    except Exception as e:
        logger.error(f"Vector Storeä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")

        # å…·ä½“çš„ãªã‚¨ãƒ©ãƒ¼å¯¾å¿œã®ææ¡ˆ
        if "authentication" in str(e).lower():
            logger.error("ğŸ”‘ APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ç’°å¢ƒå¤‰æ•°OPENAI_API_KEYãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã€‚")
        elif "quota" in str(e).lower() or "limit" in str(e).lower():
            logger.error("ğŸ’³ APIã‚¯ã‚ªãƒ¼ã‚¿ã¾ãŸã¯ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¦ã„ã¾ã™ã€‚æ–™é‡‘ãƒ—ãƒ©ãƒ³ã¾ãŸã¯ä½¿ç”¨é‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        elif "file" in str(e).lower():
            logger.error("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«é–¢é€£ã®ã‚¨ãƒ©ãƒ¼ã§ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        return None

    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        if temp_file_path and os.path.exists(temp_file_path):
            os.unlink(temp_file_path)
            logger.info("ğŸ—‘ï¸ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")


# ==================================================
# Vector Storeç®¡ç†ã‚¯ãƒ©ã‚¹
# ==================================================
class VectorStoreManager:
    """Vector Storeç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.client = OpenAI()
        self.created_stores = {}

    def process_all_datasets(self, output_dir: Path = OUTPUT_DIR) -> Dict[str, Optional[str]]:
        """
        å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‡¦ç†ã—ã¦Vector Storeã‚’ä½œæˆ

        Args:
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

        Returns:
            Dict[str, Optional[str]]: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå -> Vector Store ID ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        """
        results = {}

        logger.info("=== Vector Storeä½œæˆé–‹å§‹ ===")
        logger.info(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")

        for dataset_type, config in DATASET_CONFIGS.items():
            logger.info(f"\n--- {dataset_type} å‡¦ç†é–‹å§‹ ---")

            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®æ§‹ç¯‰
            filepath = output_dir / config["filename"]

            if not filepath.exists():
                logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
                results[dataset_type] = None
                continue

            try:
                # Step 1: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                logger.info(f"Step 1: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ - {filepath.name}")
                text_lines = load_text_file(filepath)

                if not text_lines:
                    logger.error(f"æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
                    results[dataset_type] = None
                    continue

                # Step 2: JSONLå½¢å¼ã«å¤‰æ›
                logger.info(f"Step 2: JSONLå½¢å¼å¤‰æ›")
                jsonl_data = text_to_jsonl_data(text_lines, dataset_type)

                if not jsonl_data:
                    logger.error(f"JSONLå¤‰æ›ã«å¤±æ•—: {dataset_type}")
                    results[dataset_type] = None
                    continue

                # Step 3: Vector Storeä½œæˆ
                logger.info(f"Step 3: Vector Storeä½œæˆ")
                store_name = config["store_name"]
                vector_store_id = create_vector_store_from_jsonl_data(jsonl_data, store_name)

                if vector_store_id:
                    logger.info(f"âœ… {dataset_type} Vector Storeä½œæˆæˆåŠŸ: {vector_store_id}")
                    self.created_stores[dataset_type] = vector_store_id
                    results[dataset_type] = vector_store_id
                else:
                    logger.error(f"âŒ {dataset_type} Vector Storeä½œæˆå¤±æ•—")
                    results[dataset_type] = None

            except Exception as e:
                logger.error(f"âŒ {dataset_type} å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                results[dataset_type] = None

        # çµæœã‚µãƒãƒªãƒ¼
        self._print_summary(results)
        return results

    def _print_summary(self, results: Dict[str, Optional[str]]):
        """å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        logger.info("\n=== å‡¦ç†çµæœã‚µãƒãƒªãƒ¼ ===")

        successful = {k: v for k, v in results.items() if v is not None}
        failed = {k: v for k, v in results.items() if v is None}

        logger.info(f"æˆåŠŸ: {len(successful)}/{len(results)} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
        logger.info(f"å¤±æ•—: {len(failed)}/{len(results)} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")

        if successful:
            logger.info("\nâœ… æˆåŠŸã—ãŸVector Store:")
            for dataset, store_id in successful.items():
                store_name = DATASET_CONFIGS[dataset]["store_name"]
                logger.info(f"  - {dataset}: {store_name}")
                logger.info(f"    ID: {store_id}")

        if failed:
            logger.info("\nâŒ å¤±æ•—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:")
            for dataset in failed:
                logger.info(f"  - {dataset}: {DATASET_CONFIGS[dataset]['filename']}")

    def save_results(self, results: Dict[str, Optional[str]], output_file: str = "vector_store_ids.json"):
        """çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        output_path = THIS_DIR / output_file

        # ä¿å­˜ç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        save_data = {
            "created_at"       : time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_datasets"   : len(DATASET_CONFIGS),
            "successful_stores": len([v for v in results.values() if v is not None]),
            "vector_stores"    : {}
        }

        for dataset, store_id in results.items():
            if store_id:
                save_data["vector_stores"][dataset] = {
                    "vector_store_id": store_id,
                    "store_name"     : DATASET_CONFIGS[dataset]["store_name"],
                    "description"    : DATASET_CONFIGS[dataset]["description"]
                }

        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)

            logger.info(f"ğŸ“„ çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
        except Exception as e:
            logger.error(f"çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def list_vector_stores(self) -> List[Dict]:
        """æ—¢å­˜ã®Vector Storeã‚’ä¸€è¦§è¡¨ç¤º"""
        try:
            stores = self.client.vector_stores.list()
            store_list = []

            for store in stores.data:
                store_info = {
                    "id"         : store.id,
                    "name"       : store.name,
                    "file_counts": store.file_counts.total if store.file_counts else 0,
                    "created_at" : store.created_at,
                    "usage_bytes": store.usage_bytes
                }
                store_list.append(store_info)

            return store_list
        except Exception as e:
            logger.error(f"Vector Storeä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []


# ==================================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†
# ==================================================
def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸš€ Vector Storeä½œæˆãƒ—ãƒ­ã‚°ãƒ©ãƒ é–‹å§‹")

    # ç’°å¢ƒç¢ºèª
    if not os.getenv("OPENAI_API_KEY"):
        logger.error("âŒ OPENAI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    if not OUTPUT_DIR.exists():
        logger.error(f"âŒ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {OUTPUT_DIR}")
        logger.info("ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™:")
        for config in DATASET_CONFIGS.values():
            logger.info(f"  - {OUTPUT_DIR / config['filename']}")
        return

    # å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    missing_files = []
    for dataset_type, config in DATASET_CONFIGS.items():
        filepath = OUTPUT_DIR / config["filename"]
        if not filepath.exists():
            missing_files.append(filepath)

    if missing_files:
        logger.error("âŒ ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:")
        for filepath in missing_files:
            logger.error(f"  - {filepath}")
        return

    # Vector Store Manager ã‚’ä½œæˆã—ã¦å‡¦ç†å®Ÿè¡Œ
    manager = VectorStoreManager()

    try:
        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‡¦ç†
        results = manager.process_all_datasets()

        # çµæœã‚’ä¿å­˜
        manager.save_results(results)

        # æ—¢å­˜ã®Vector Storeä¸€è¦§ã‚‚è¡¨ç¤º
        logger.info("\n=== æ—¢å­˜Vector Storeä¸€è¦§ ===")
        existing_stores = manager.list_vector_stores()
        if existing_stores:
            for store in existing_stores[:10]:  # æœ€æ–°10ä»¶
                logger.info(f"  {store['name']}: {store['id']}")
        else:
            logger.info("  Vector StoreãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        raise

    logger.info("ğŸ Vector Storeä½œæˆãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†")


# ==================================================
# å€‹åˆ¥å®Ÿè¡Œç”¨é–¢æ•°
# ==================================================
def create_single_vector_store(dataset_type: str, output_dir: Path = OUTPUT_DIR) -> Optional[str]:
    """
    å˜ä¸€ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰Vector Storeã‚’ä½œæˆ

    Args:
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

    Returns:
        Vector Store IDï¼ˆæˆåŠŸæ™‚ï¼‰ã¾ãŸã¯Noneï¼ˆå¤±æ•—æ™‚ï¼‰
    """
    if dataset_type not in DATASET_CONFIGS:
        logger.error(f"æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—: {dataset_type}")
        return None

    config = DATASET_CONFIGS[dataset_type]
    filepath = output_dir / config["filename"]

    if not filepath.exists():
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
        return None

    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        text_lines = load_text_file(filepath)

        # JSONLå½¢å¼ã«å¤‰æ›
        jsonl_data = text_to_jsonl_data(text_lines, dataset_type)

        # Vector Storeä½œæˆ
        store_name = config["store_name"]
        vector_store_id = create_vector_store_from_jsonl_data(jsonl_data, store_name)

        return vector_store_id

    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
        return None


if __name__ == "__main__":
    main()
