# python a30_00_rag_dataset_from_huggingface.py
# ä¸‹è¨˜ã‚³ãƒ¼ãƒ‰ã¯ã€ãƒã‚¤ã‚ºãŒå¤§ãã„ã€‚ ---> 09_01_rag_
# --------------------------------------------------
# â‘  ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ»FAQãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ   æ¨å¥¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼š Amazon_Polarity
# â‘¡ ä¸€èˆ¬çŸ¥è­˜ãƒ»ãƒˆãƒªãƒ“ã‚¢QAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ      æ¨å¥¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼š trivia_qa
# â‘¢ åŒ»ç™‚è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ             æ¨å¥¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼š FreedomIntelligence/medical-o1-reasoning-SFT
# â‘£ ç§‘å­¦ãƒ»æŠ€è¡“QAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ             æ¨å¥¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼š sciq
# â‘¤ æ³•å¾‹ãƒ»åˆ¤ä¾‹QAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ             æ¨å¥¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼š nguha/legalbench
# --------------------------------------------------
# Embeddingã®å‰å‡¦ç†ï¼šã€€1è¡Œ1ãƒ™ã‚¯ãƒˆãƒ«ã«ãªã‚‹å½¢ãŒç†æƒ³
# --------------------------------------------------
import os
import re
import time
import json
from pathlib import Path
from typing import List, Optional
import logging

from openai import OpenAI
from openai.types.chat import ChatCompletionMessageParam, ChatCompletionSystemMessageParam, \
    ChatCompletionUserMessageParam, ChatCompletionAssistantMessageParam

from datasets import load_dataset
import pandas as pd
import tempfile
import textwrap
from pydantic import BaseModel, Field
from tqdm import tqdm

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from helper_st import (
        UIHelper, MessageManagerUI, ResponseProcessorUI,
        SessionStateManager, error_handler_ui, timer_ui,
        init_page, select_model, InfoPanelManager
    )
    from helper_api import (
        config, logger, TokenManager, OpenAIClient,
        EasyInputMessageParam, ResponseInputTextParam,
        ConfigManager, MessageManager, sanitize_key,
        error_handler, timer
    )
except ImportError as e:
    # StreamlitãŒåˆ©ç”¨ã§ããªã„ç’°å¢ƒã§ã®å¯¾å¿œ
    print(f"ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
    pass

BASE_DIR = Path(__file__).resolve().parent.parent       # Paslib
THIS_DIR = Path(__file__).resolve().parent              # Paslib

# ----------------------------------------------------
# 1. Customer Support FAQs/ FAQå‹ã®ãƒ‡ãƒ¼ã‚¿ï¼š
# ----------------------------------------------------
def clean_customer_support_faq(csv_path):
    DATASETS_DIR = Path(THIS_DIR) / "datasets"
    csv_path = DATASETS_DIR / "customer_support_faq.csv"
    df = pd.read_csv(csv_path)
    print(df.head(10))

    tmp_txt = tempfile.NamedTemporaryFile(delete=False, suffix=".txt")
    with open(tmp_txt.name, "w", encoding="utf-8") as f:
        for _, row in df.iterrows():
            f.write(textwrap.dedent(f"""\
                Q: {row['question']}
                A: {row['answer']}
            """))
    print("plain-text file:", tmp_txt.name)
    return tmp_txt.name

# 1) å–ã‚Šå‡ºã—ãŸã„æ§‹é€ ã‚’ Pydantic ã§å®£è¨€ --------------------
class FaqInfo(BaseModel):
    # æŠ½å‡ºå¯¾è±¡ï¼šanswer
    faq_answer: List[str] = Field(..., description="Faq Answer")

# ----------------------------------------------------
# 1. (main)Customer Support FAQs/ FAQå‹ã®ãƒ‡ãƒ¼ã‚¿ï¼š
# ----------------------------------------------------
def customer_support_faq_main():
    # download_dataset()
    # ----------------------------------------------------
    # 1. Customer Support FAQs/ FAQå‹ã®ãƒ‡ãƒ¼ã‚¿ï¼š
    # ----------------------------------------------------
    # csv_path = os.path.join(DATASETS_DIR, "customer_support_faq.csv")
    # print("(1) csv_path:", csv_path)
    #
    # plain_txt_path = set_dataset_to_qa(csv_path)
    # vs_id = create_vector_store_and_upload(plain_txt_path)

    print("(3) Search results:")
    # query_text = "What is your return policy? "
    # query_text = "I want to confirm my order."
    query_text = "I want to check where my shipped package is."
    vs_id = 'vs_684f6202dbe48191bb897b0772563020'
    res_text = standalone_search(query_text, vs_id)
    print("\n\n(3) Search results:-------------------", res_text)

    client = OpenAI()
    model = "gpt-4o-mini"  # ãƒ¢ãƒ‡ãƒ«åã‚’ä¿®æ­£
    messages = f"find and display the answer to the question {query_text} from the following Q: A: information. Information: {res_text}"
    res = client.responses.create(model=model, input=messages)

    print("============")
    for i, txt in enumerate(extract_text_from_response(res), 1):
        print(f"{i}: {txt}\n")

    print("============")
    response = client.responses.parse(model=model, input=messages, text_format=FaqInfo, )
    faq_info: FaqInfo = response.output_parsed
    print(faq_info.model_dump())

# ----------------------------------------------------
# 2. Legal QA â€” *consumer_contracts_qa
# åˆ—:     Question,Complex_CoT,Response
# å‰å‡¦ç†ï¼š
# ãƒ»ã€Œä½•ã®ãŸã‚ã«æ¤œç´¢ãƒ»é¡ä¼¼åº¦è¨ˆç®—ã™ã‚‹ã®ã‹ã€ã‚’å›ºã‚ã€åˆ—ã”ã¨ã®å½¹å‰²ã‚’æ•´ç†ã™ã‚‹ã“ã¨ã€‚
# ãƒ»Complex_CoT ã¯ãã®ã¾ã¾åŸ‹ã‚è¾¼ã‚€ã¨ãƒã‚¤ã‚ºæºã«ãªã‚Šã‚„ã™ã„ã€‚è¦ç´„ã‹é™¤å¤–ãŒç„¡é›£
# ãƒ»åŒ»ç™‚ QA ã§ã¯ç”¨èªã‚†ã‚Œãƒ»ç•¥èªã‚†ã‚ŒãŒæ¤œç´¢ç²¾åº¦ã‚’è½ã¨ã—ã‚„ã™ã„ãŸã‚ã€æ­£è¦åŒ–è©å…¸ã‚’ã‚‚ã¤ã¨åŠ¹æœå¤§ã€‚
# ãƒ»å®Ÿé‹ç”¨å‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ è¿‘å‚æ¤œç´¢â†’äººæ‰‹æ¤œè¨¼ ã‚’è¡Œã„ã€å‰å‡¦ç†ã®éä¸è¶³ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹
# ----------------------------------------------------
def set_dataset_02(csv_path):
    DATASETS_DIR = Path(THIS_DIR) / "datasets"
    csv_path = DATASETS_DIR / "medical_qa.csv"
    print("(1) csv_path:", csv_path)
    df = pd.read_csv(csv_path)

    # 1. è»½ã„ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ä¾‹
    df["Question"] = df["Question"].str.strip()
    df["Response"] = df["Response"].str.strip()

    # 2. CoT è¦ç´„ or é™¤å»ï¼ˆã“ã“ã§ã¯é™¤å»ï¼‰
    df["Clean_CoT"] = df["Complex_CoT"].str.replace(
        r"(?i)^okay,.*?\n", "", regex=True  # å°å…¥å¥ãªã©ã‚’ã–ã£ãã‚Šå‰Šé™¤
    )

    # 3. æ–‡æ›¸ç”Ÿæˆï¼ˆã“ã“ã§ã¯ Strategyâ‘ ï¼‰
    df["doc_for_embed"] = df["Question"] + "\n\n" + df["Response"]

    # 4. ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    client = OpenAI()
    df["embedding"] = df["doc_for_embed"].apply(
        lambda x: client.embeddings.create(model="text-embedding-3-large", input=x).data[0].embedding
    )

def legal_qa_main():
    csv_path = "medical_qa.csv"
    set_dataset_02(csv_path)

# ----------------------------------------------------
# 3. Medical QA â€” *medical-o1-reasoning-SFT
#    å‰å‡¦ç†ï¼š
# ----------------------------------------------------
# ========= ãƒ‘ã‚¹è¨­å®šã‚’ pathlib.Path ã§çµ±ä¸€ =========
# BASE_DIR = Path(__file__).resolve().parent
ROOT_DIR = Path(__file__).resolve().parent.parent
DATASETS_DIR = ROOT_DIR / "datasets"
INPUT_CSV: Path = DATASETS_DIR / "medical_qa.csv"
OUTPUT_CSV: Path = DATASETS_DIR / "medical_qa_summarized.csv"
OUTPUT_CLEAN_CSV: Path = DATASETS_DIR / "medical_qa_clean.csv"

# ========= è¦ç´„é–¢æ•° =========
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential())
def summarize_cot(cot_text: str) -> str:
    # æœ€æ–° SDK ã§ã¯ chat.completions.create ã‚’ä½¿ç”¨
    client = OpenAI()
    # system_content = 'ã‚ãªãŸã¯åŒ»ç™‚æƒ…å ±ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®åŒ»ç™‚çš„ãªæ¨è«–å†…å®¹ã‚’ç°¡æ½”ã«è¦ç´„ã—ã€è¨ºæ–­ã‚„åˆ¤æ–­ã®è¦ç‚¹ãŒæ˜ç¢ºã«ãªã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚'
    # assistant_content = 'åŒ»ç™‚æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã‚’ã€åŒ»å­¦çš„ã«é‡è¦ãªè¦ç‚¹ã®ã¿ã‚’æŠœç²‹ã—100æ–‡å­—ä»¥å†…ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚'
    system_content = 'You are a medical information expert. Please summarize the following medical reasoning content concisely, making the key points of diagnosis and judgment clear.'
    assistant_content = 'Please extract only the medically important key points from the medical reasoning process and summarize them within 100 characters.'
    user_content = cot_text
    # {"role": "system", "content": system_content},
    messages = [
        ChatCompletionSystemMessageParam(role="system", content=system_content),
        ChatCompletionUserMessageParam(role="user", content=user_content),
        ChatCompletionAssistantMessageParam(role="assistant", content=assistant_content),
    ]
    resp = client.chat.completions.create(
        model="gpt-4o-mini",            # ãƒ¢ãƒ‡ãƒ«åã‚’ä¿®æ­£
        messages=messages,
        temperature=0,
    )
    return resp.choices[0].message.content.strip()

def medical_qa_main():
    print("medical_qa_main start ...")
    if not INPUT_CSV.exists():
        raise FileNotFoundError(f"{INPUT_CSV} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    df = pd.read_csv(INPUT_CSV)
    # æœ€ä½é™ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    df["Question"] = df["Question"].str.strip()
    df["Response"] = df["Response"].str.strip()

    # (A) Complex_CoT å†…éƒ¨ã®ç©ºè¡Œã‚’é™¤å» â˜…è¿½åŠ 
    df["Complex_CoT"] = (
        df["Complex_CoT"]
        .astype(str)  # NaN å¯¾ç­–ã§ str åŒ–
        .apply(lambda s: re.sub(r"\n\s*\n+", "\n", s).strip())
    )

    # (B) è¡Œå…¨ä½“ãŒç©ºã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’é™¤å»ï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
    df.replace(r"^\s*$", pd.NA, regex=True, inplace=True)
    df.dropna(subset=["Question", "Complex_CoT", "Response"], how="all", inplace=True)
    df.reset_index(drop=True, inplace=True)

    # (3) ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®CSVã‚’ä¿å­˜
    OUTPUT_CLEAN_CSV.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(OUTPUT_CLEAN_CSV, index=False)
    print(f"[OK] cleaned file saved âœ {OUTPUT_CLEAN_CSV}")

    # Complex_CoT ã‚’è¦ç´„ (rateâ€‘limit å¯¾ç­–ã§å¿…è¦ãªã‚‰ chunksize/batch ã”ã¨ã«å‡¦ç†)
    # df["CoT"] = df["Complex_CoT"].apply(summarize_cot)
    # tqdm.pandas(desc="Summarizing CoT")
    # df["CoT"] = df["Complex_CoT"].progress_apply(summarize_cot)

    from tqdm import tqdm  # tqdm ãŒæœª import ã®å ´åˆã¯è¿½åŠ 
    # é€²æ—ãƒãƒ¼ã‚’ç™»éŒ²ï¼ˆã‚¿ã‚¤ãƒˆãƒ«å¤‰æ›´ï¼‰
    tqdm.pandas(desc="Summarizing CoT (top-100 rows)")

    # â‘  ã¾ãš CoT åˆ—ã‚’ç©ºã§ä½œæˆ
    df["CoT"] = pd.NA

    # â‘¡ ä¸Šã‹ã‚‰ 100 è¡Œã ã‘è¦ç´„ã‚’å®Ÿè¡Œ
    df.loc[:99, "CoT"] = df.loc[:99, "Complex_CoT"].progress_apply(summarize_cot)

    # å‡ºåŠ›
    print("\ndf_result:------------\n")
    df_result = df[["Question", "CoT", "Response"]]
    print(df_result.head())

    OUTPUT_CSV.parent.mkdir(exist_ok=True, parents=True)
    df_result.to_csv(OUTPUT_CSV, index=False)
    print(f"[OK] summarized file saved âœ {OUTPUT_CSV}")

# ----------------------------------------------------
# INPUT-Docã‚’ãã®ã¾ã¾ä½¿ã†ã€embeddingã—ã¦ã€vector storeã«ä¿å­˜ã€‚
# ----------------------------------------------------
INPUT_CLEAN_CSV: Path = DATASETS_DIR / "medical_qa_clean.csv"

def medical_qa_main_short_cut():
    print("medical_qa_main_short_cut start ...")
    if not INPUT_CLEAN_CSV.exists():
        raise FileNotFoundError(f"{INPUT_CLEAN_CSV} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    df = pd.read_csv(INPUT_CLEAN_CSV)

# ----------------------------------------------------
# medical_qa_search:
#  - vector store ã¸ç™»éŒ²
#    æ¤œç´¢
# ----------------------------------------------------
from tempfile import NamedTemporaryFile

def medical_qa_make_vector() -> None:
    print("medical_qa_search start ...")
    # vs_id = 'vs_6852c5fff27c8191a08c0b0b936a1d71'

    # â–¼ â‘  ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ CSV ã®å­˜åœ¨ç¢ºèª
    if not OUTPUT_CLEAN_CSV.exists():
        raise FileNotFoundError(f"{OUTPUT_CLEAN_CSV} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    # â–¼ â‘¡ ãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼
    df = pd.read_csv(OUTPUT_CLEAN_CSV)

    # -------------------------------------------------------------
    # (1)  Embedding ç”¨ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—
    #      å½¢å¼ï¼š  Q: ...\\nA: ...\\n\\n
    # -------------------------------------------------------------
    tmp_txt = NamedTemporaryFile(delete=False, suffix=".txt", mode="w", encoding="utf-8")
    up_text = ''
    with tmp_txt as fh:
        for _, row in df.iterrows():
            fh.write(f"Q: {row['Question']}\nA: {row['Response']}\n\n")
            up_text = up_text + f"Q: {row['Question']}\nA: {row['Response']}\n\n"

    # -------------------------------------------------------------
    # (2) Vector Store ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè‡ªå‰ util ã‚’å†åˆ©ç”¨ï¼‰
    # -------------------------------------------------------------
    upload_name = "medical_store_jp"
    vs_id = create_vector_store_and_upload(up_text, upload_name)

# -------------------------------------------------------------
# (3) RAG æ¤œç´¢ãƒ†ã‚¹ãƒˆï¼ˆ5 å•ï¼‰
#     â†’ Responses API ã‚’ Retrieval ãƒ¢ãƒ¼ãƒ‰ã§å‘¼ã¶
# -------------------------------------------------------------
def medical_qa_search(vs_id):
    # vs_id = "vs_6852c5fff27c8191a08c0b0b936a1d71"
    questions = [
        "After long-distance travel, a patient presents with swelling and tenderness in the right lower leg and sudden weakness in the left arm and leg. What cardiac abnormality is most likely to explain these findings?",
        "A 5-cm stab wound is located at the upper border of the 8th rib along the left mid-axillary line. Which thoracic structure is most likely to be injured?",
        "In a 61-year-old woman with normal bladder function who complains of stress urinary incontinence, which diagnostic test is the most useful?",
        "A 45-year-old man with a history of chronic heavy alcohol use suddenly develops dysarthria and tremor. What is the most likely diagnosis?",
        "Which disorder, characterized by parkinsonian features and cognitive impairment, is primarily associated with deposition of Lewy bodies?",
    ]

    answer5 = (
        "A1 é–‹å­˜åµå††å­”ï¼ˆPFOï¼‰ã€‚æ·±éƒ¨é™è„ˆè¡€æ “ãŒå³æˆ¿ â†’ å·¦æˆ¿ã¸çŸ­çµ¡ã—ã€è„³å¡æ “ã‚’èµ·ã“ã™ paradoxical embolism ãŒæ©Ÿåºã€‚\n"
        "A2 å·¦è‚ºä¸‹è‘‰ï¼ˆlower lobe of the left lungï¼‰ã€‚åˆºå‰µæ·±åº¦ã¨è§£å‰–å­¦çš„ä½ç½®ã‹ã‚‰è¡€èƒ¸ãƒ»æ°—èƒ¸ã‚’ä¼´ã†è‚ºæå‚·ãŒæœ€ã‚‚ç–‘ã‚ã‚Œã‚‹ã€‚\n"
        "A3 è†€èƒ±å†…åœ§æ¸¬å®šï¼ˆcystometryï¼‰ã€‚å°¿é“æ”¯æŒæ©Ÿæ§‹ã®éšœå®³ã‚’è©•ä¾¡ã—ã€è…¹åœ§æ€§å°¿å¤±ç¦ã‚’ç¢ºèªã§ãã‚‹ã€‚\n"
        "A4 ç²å¾—æ€§è‚è„³å¤‰æ€§ï¼ˆacquired hepatocerebral degenerationï¼‰ã€‚æ…¢æ€§è‚ç–¾æ‚£ã«ä¼´ã†ä¸­æ¢ç¥çµŒéšœå®³ã§ã€æ€¥æ€§ã®é‹å‹•å¤±èª¿ãƒ»æŒ¯æˆ¦ã‚’æ¥ã™ã€‚\n"
        "A5 ãƒ¬ãƒ“ãƒ¼å°ä½“å‹èªçŸ¥ç—‡ï¼ˆDementia with Lewy bodiesï¼‰ã¾ãŸã¯ Lewy å°ä½“ãƒ‘ãƒ¼ã‚­ãƒ³ã‚½ãƒ³ç—…ã€‚Lewy å°ä½“ã®å­˜åœ¨ãŒè¨ºæ–­ã®éµã€‚\n"
        "A1 Patent foramen ovale (PFO). A deep-vein thrombus can shunt from the right atrium to the left atrium, producing a paradoxical embolism that reaches the brain.\n"
        "A2 Lower lobe of the left lung. Given the stab depth and anatomical location, a lung injury with accompanying hemothorax or pneumothorax is most likely.\n"
        "A3 Cystometry (intravesical pressure measurement). Assesses urethral support dysfunction and confirms stress urinary incontinence.\n"
        "A4 Acquired hepatocerebral degeneration. A central-nervous disorder associated with chronic liver disease that causes acute ataxia and tremor.\n"
        "A5 Dementia with Lewy bodies (DLB) or Lewy-body Parkinson's disease. The presence of Lewy bodies is the diagnostic key.\n"
    )

    q1 = "After long-distance travel, a patient presents with swelling and tenderness in the right lower leg and sudden weakness in the left arm and leg. What cardiac abnormality is most likely to explain these findings?"
    a1 = standalone_search(vs_id, q1)

    print("\n======= RAG QA Test =======")
    for i, q in enumerate(questions, 1):
        # Responses ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’æŒ‡å®š
        answer = standalone_search(vs_id, q)
        print(f"\nQ{i}: {q}\nA{i}: {answer}\n{'-'*60}")

    print("\nanswer:=", answer5)

# ----------------------------------------------------
# 4. SciQ â€” Science MCQ
#    å‰å‡¦ç†ï¼š
# ----------------------------------------------------
def set_dataset_04(csv_path):
    pass

def sci_qa_main():
    pass

# ----------------------------------------------------
# 5. Trivia QA â€” *rc*ï¼ˆReading Comprehensionï¼‰
#    å‰å‡¦ç†ï¼š
# ----------------------------------------------------
def set_dataset_05(csv_path):
    pass

# ----------------------------------------------------
# â‘  ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ»FAQãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ   æ¨å¥¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼š Amazon_Polarity
# ----------------------------------------------------
# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†

def clean_text(text):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†
    - æ”¹è¡Œã®é™¤å»
    - é€£ç¶šã—ãŸç©ºç™½ã‚’1å€‹ã®ç©ºç™½ã«ã¾ã¨ã‚ã‚‹
    """
    if pd.isna(text):
        return ""

    # æ”¹è¡Œã®é™¤å»
    text = re.sub(r'\n+', ' ', str(text))

    # é€£ç¶šã—ãŸç©ºç™½ã‚’1å€‹ã®ç©ºç™½ã«ã¾ã¨ã‚ã‚‹
    text = re.sub(r'\s+', ' ', text)

    # å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
    text = text.strip()

    return text


def get_embeddings_batch(texts: List[str], model: str = "text-embedding-3-small", batch_size: int = 100) -> List[
    Optional[List[float]]]:
    """
    OpenAI Embedding APIã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã‚’ãƒãƒƒãƒã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–

    Args:
        texts: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        model: ä½¿ç”¨ã™ã‚‹embeddingãƒ¢ãƒ‡ãƒ« (æ¨å¥¨: text-embedding-3-small)
        batch_size: ä¸€åº¦ã®APIå‘¼ã³å‡ºã—ã§å‡¦ç†ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆæ•°ï¼ˆæœ€å¤§2048ï¼‰

    Returns:
        åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆ
    """
    client = OpenAI()
    embeddings = []

    # ãƒ†ã‚­ã‚¹ãƒˆã‚’å‰å‡¦ç†ï¼ˆæ”¹è¡Œæ–‡å­—ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›ï¼‰
    cleaned_texts = [text.replace("\n", " ") for text in texts]

    logger.info(f"Embeddingä½œæˆé–‹å§‹: {len(cleaned_texts)}ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’{batch_size}ä»¶ãšã¤å‡¦ç†")

    # ãƒãƒƒãƒã”ã¨ã«å‡¦ç†
    for i in tqdm(range(0, len(cleaned_texts), batch_size), desc="Embeddingä½œæˆä¸­"):
        batch = cleaned_texts[i:i + batch_size]

        try:
            response = client.embeddings.create(
                input=batch,
                model=model,
                # dimensions=1024  # ã‚³ã‚¹ãƒˆåŠ¹ç‡ã‚’é‡è¦–ã™ã‚‹å ´åˆã¯æ¬¡å…ƒæ•°ã‚’å‰Šæ¸›
            )

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰embeddingã‚’å–å¾—
            batch_embeddings = [data.embedding for data in response.data]
            embeddings.extend(batch_embeddings)

            logger.info(
                f"ãƒãƒƒãƒ {i // batch_size + 1}/{(len(cleaned_texts) - 1) // batch_size + 1}: {len(batch)}ä»¶å‡¦ç†å®Œäº†")

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œã®ãŸã‚ã®çŸ­ã„å¾…æ©Ÿ
            time.sleep(0.1)

        except Exception as e:
            logger.error(f"ãƒãƒƒãƒ {i // batch_size + 1}ã§ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯Noneã§åŸ‹ã‚ã‚‹
            embeddings.extend([None] * len(batch))

            # ä¸€æ™‚çš„ãªå•é¡Œã®å ´åˆã¯ãƒªãƒˆãƒ©ã‚¤
            time.sleep(2)

    return embeddings


def create_vector_store_from_dataframe(df_clean: pd.DataFrame, store_name: str = "Customer Support FAQ") -> Optional[str]:
    """
    DataFrameã‹ã‚‰Vector Storeã‚’ä½œæˆï¼ˆä¿®æ­£ç‰ˆï¼šå‹ã‚¨ãƒ©ãƒ¼å¯¾å¿œï¼‰

    Args:
        df_clean: ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ¸ˆã¿ã®DataFrame
        store_name: Vector Storeã®åå‰

    Returns:
        Vector Store IDï¼ˆæˆåŠŸæ™‚ï¼‰ã¾ãŸã¯Noneï¼ˆå¤±æ•—æ™‚ï¼‰
    """
    client = OpenAI()
    temp_file_path = None
    uploaded_file_id = None

    try:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™
        # æ‹¡å¼µå­ã‚’.txtã«å¤‰æ›´ï¼ˆOpenAI Files APIã®åˆ¶é™å¯¾å¿œï¼‰
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as temp_file:
            for idx, row in df_clean.iterrows():
                # JSONLå½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãè¾¼ã¿ï¼ˆæ‹¡å¼µå­ã¯.txtã ãŒä¸­èº«ã¯JSONLï¼‰
                json_line = {
                    "id"  : f"faq_{idx}",
                    "text": row['combined_text']
                }
                temp_file.write(json.dumps(json_line, ensure_ascii=False) + '\n')

            temp_file_path = temp_file.name

        logger.info(f"JSONLãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {temp_file_path}")

        # Step 1: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’OpenAIã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆ.txtæ‹¡å¼µå­ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰
        with open(temp_file_path, 'rb') as file:
            uploaded_file = client.files.create(
                file=file,
                purpose="assistants"
            )
            uploaded_file_id = uploaded_file.id

        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: File ID={uploaded_file_id}")

        # Step 2: Vector Storeã‚’ä½œæˆï¼ˆå•é¡Œã®ã‚ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼‰
        vector_store = client.vector_stores.create(
            name=store_name,
            # expires_after ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼ˆå‹ã‚¨ãƒ©ãƒ¼å¯¾å¿œï¼‰
            # chunking_strategy ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼ˆå‹ã‚¨ãƒ©ãƒ¼å¯¾å¿œï¼‰
            metadata={
                "created_by" : "customer_support_faq_processor",
                "version"    : "2025.1",
                "data_format": "jsonl_as_txt"
            }
        )

        logger.info(f"Vector Storeä½œæˆå®Œäº†: ID={vector_store.id}")

        # Step 3: Vector Storeã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Linkã™ã‚‹ï¼ˆchunking_strategyå‰Šé™¤ï¼‰
        vector_store_file = client.vector_stores.files.create(
            vector_store_id=vector_store.id,
            file_id=uploaded_file_id
            # chunking_strategy ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼ˆå‹ã‚¨ãƒ©ãƒ¼å¯¾å¿œï¼‰
        )

        logger.info(f"Vector StoreFileãƒªãƒ³ã‚¯ä½œæˆ: {vector_store_file.id}")

        # Step 4: ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†ã‚’å¾…æ©Ÿ
        max_wait_time = 300  # æœ€å¤§5åˆ†å¾…æ©Ÿ
        wait_interval = 5  # 5ç§’é–“éš”ã§ãƒã‚§ãƒƒã‚¯
        waited_time = 0

        while waited_time < max_wait_time:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ç¢ºèª
            file_status = client.vector_stores.files.retrieve(
                vector_store_id=vector_store.id,
                file_id=uploaded_file_id
            )

            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†çŠ¶æ³: {file_status.status} (å¾…æ©Ÿæ™‚é–“: {waited_time}ç§’)")

            if file_status.status == "completed":
                # Vector Storeå…¨ä½“ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ç¢ºèª
                updated_vector_store = client.vector_stores.retrieve(vector_store.id)

                logger.info(f"âœ… Vector Storeä½œæˆå®Œäº†:")
                logger.info(f"  - ID: {vector_store.id}")
                logger.info(f"  - Name: {vector_store.name}")
                logger.info(f"  - ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†çŠ¶æ³: {file_status.status}")
                logger.info(f"  - ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {updated_vector_store.file_counts.total}")
                logger.info(f"  - å®Œäº†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {updated_vector_store.file_counts.completed}")
                logger.info(f"  - å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {updated_vector_store.file_counts.failed}")
                logger.info(f"  - ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡: {updated_vector_store.usage_bytes} bytes")

                return vector_store.id

            elif file_status.status == "failed":
                logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¤±æ•—: {file_status.last_error}")
                return None

            elif file_status.status in ["in_progress", "cancelling"]:
                # å‡¦ç†ä¸­ã®å ´åˆã¯ç¶™ç¶šã—ã¦å¾…æ©Ÿ
                time.sleep(wait_interval)
                waited_time += wait_interval
            else:
                logger.warning(f"âš ï¸ äºˆæœŸã—ãªã„ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {file_status.status}")
                time.sleep(wait_interval)
                waited_time += wait_interval

        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã®å ´åˆ
        logger.error(f"âŒ Vector Storeä½œæˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (åˆ¶é™æ™‚é–“: {max_wait_time}ç§’)")
        return None

    except Exception as e:
        logger.error(f"Vector Storeä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")

        # å…·ä½“çš„ãªã‚¨ãƒ©ãƒ¼å¯¾å¿œã®ææ¡ˆ
        if "authentication" in str(e).lower():
            logger.error("ğŸ”‘ APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ç’°å¢ƒå¤‰æ•°OPENAI_API_KEYãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã€‚")
        elif "quota" in str(e).lower() or "limit" in str(e).lower():
            logger.error("ğŸ’³ APIã‚¯ã‚ªãƒ¼ã‚¿ã¾ãŸã¯ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¦ã„ã¾ã™ã€‚æ–™é‡‘ãƒ—ãƒ©ãƒ³ã¾ãŸã¯ä½¿ç”¨é‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        elif "file" in str(e).lower():
            logger.error("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«é–¢é€£ã®ã‚¨ãƒ©ãƒ¼ã§ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        elif "extension" in str(e).lower() or "format" in str(e).lower():
            logger.error("ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å•é¡Œã§ã™ã€‚ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        return None

    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        if temp_file_path and os.path.exists(temp_file_path):
            os.unlink(temp_file_path)
            logger.info("ğŸ—‘ï¸ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")


def validate_embeddings(df_clean: pd.DataFrame) -> bool:
    """
    Embeddingãƒ‡ãƒ¼ã‚¿ã®å“è³ªã‚’æ¤œè¨¼

    Args:
        df_clean: Embeddingä»˜ãã®DataFrame

    Returns:
        æ¤œè¨¼çµæœï¼ˆTrue: æ­£å¸¸, False: å•é¡Œã‚ã‚Šï¼‰
    """
    if 'embedding' not in df_clean.columns:
        logger.error("embeddingåˆ—ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        return False

    null_count = df_clean['embedding'].isnull().sum()
    total_count = len(df_clean)
    success_rate = (total_count - null_count) / total_count * 100

    logger.info(f"Embeddingå“è³ªæ¤œè¨¼:")
    logger.info(f"  - ç·ãƒ‡ãƒ¼ã‚¿æ•°: {total_count}")
    logger.info(f"  - æˆåŠŸæ•°: {total_count - null_count}")
    logger.info(f"  - å¤±æ•—æ•°: {null_count}")
    logger.info(f"  - æˆåŠŸç‡: {success_rate:.1f}%")

    # æˆåŠŸç‡ãŒ90%æœªæº€ã®å ´åˆã¯è­¦å‘Š
    if success_rate < 90:
        logger.warning(f"EmbeddingæˆåŠŸç‡ãŒ{success_rate:.1f}%ã¨ä½ã„ã§ã™ã€‚APIã‚­ãƒ¼ã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return False

    return True

# ===
# ----------------------------------------------------
# 1. Customer Support FAQs/ FAQå‹ã®ãƒ‡ãƒ¼ã‚¿ï¼š
# ----------------------------------------------------
def make_vs_id_customer_support_faq():
    logger.info("=== OpenAI APIæœ€æ–°ç‰ˆ Vector Storeä½œæˆå‡¦ç†é–‹å§‹ ===")

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    DATASETS_DIR = Path(THIS_DIR) / "datasets"
    csv_path = DATASETS_DIR / "customer_support_faq.csv"

    if not csv_path.exists():
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
        return

    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    df = pd.read_csv(csv_path)

    logger.info(f"å…ƒãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†:")
    logger.info(f"  - è¡Œæ•°: {len(df)}")
    logger.info(f"  - åˆ—æ•°: {len(df.columns)}")
    logger.info(f"  - åˆ—å: {list(df.columns)}")

    # df_cleanã‚’æ–°ã—ãå®šç¾©ï¼ˆã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†ï¼‰
    logger.info("ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°é–‹å§‹...")
    df_clean = pd.DataFrame({
        'combined_text': df.apply(
            lambda row: clean_text(str(row['question']) + ' ' + str(row['answer'])),
            axis=1
        )
    })

    logger.info(f"ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿:")
    logger.info(f"  - è¡Œæ•°: {len(df_clean)}")
    logger.info(f"  - åˆ—æ•°: {len(df_clean.columns)}")
    logger.info(f"  - åˆ—å: {list(df_clean.columns)}")

    # çµæœã®ç¢ºèªç”¨ï¼ˆæœ€åˆã®3è¡Œã‚’è¡¨ç¤ºï¼‰
    logger.info("=== å‡¦ç†çµæœã®ç¢ºèª ===")
    for i in range(min(3, len(df_clean))):
        logger.info(f"ã€è¡Œ {i + 1}ã€‘")
        logger.info(f"  å…ƒã®question: {df.iloc[i]['question'][:100]}...")
        logger.info(f"  å…ƒã®answer: {df.iloc[i]['answer'][:100]}...")
        logger.info(f"  é€£çµãƒ»ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œ: {df_clean.iloc[i]['combined_text'][:200]}...")

    # Embeddingã®ä½œæˆï¼ˆãƒãƒƒãƒå‡¦ç†ã§åŠ¹ç‡åŒ–ï¼‰
    logger.info("=== Embeddingä½œæˆé–‹å§‹ ===")
    logger.info(f"å‡¦ç†å¯¾è±¡: {len(df_clean)}ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆ")

    # ãƒãƒƒãƒå‡¦ç†ã§embeddingã‚’ä½œæˆ
    embeddings = get_embeddings_batch(
        df_clean['combined_text'].tolist(),
        model='text-embedding-3-small',  # ã‚³ã‚¹ãƒˆåŠ¹ç‡é‡è¦–
        batch_size=50  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ã—ã¦50ä»¶ãšã¤å‡¦ç†
    )

    # çµæœã‚’DataFrameã«è¿½åŠ 
    df_clean['embedding'] = embeddings

    # Embeddingå“è³ªã®æ¤œè¨¼
    if not validate_embeddings(df_clean):
        logger.warning("Embeddingã®å“è³ªã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™ãŒã€çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    # ä½œæˆã—ãŸembeddingãƒ‡ãƒ¼ã‚¿ã‚’CSVã«ä¿å­˜
    output_dir = Path(THIS_DIR) / "output"
    output_dir.mkdir(exist_ok=True)
    output_path = output_dir / "customer_support_faq_embedded.csv"

    try:
        # Embeddingãƒ‡ãƒ¼ã‚¿ã¯æ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜ï¼ˆCSVã®åˆ¶é™å¯¾å¿œï¼‰
        df_save = df_clean.copy()
        df_save['embedding'] = df_save['embedding'].apply(lambda x: str(x) if x is not None else None)
        df_save.to_csv(output_path, index=False)
        logger.info(f"Embeddingãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {output_path}")
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    # Vector Storeã®ä½œæˆ
    logger.info("=== Vector Storeä½œæˆé–‹å§‹ ===")
    vector_store_id = create_vector_store_from_dataframe(df_clean, "Customer Support FAQ v2025")

    if vector_store_id:
        logger.info(f"ğŸ‰ Vector Storeä½œæˆæˆåŠŸ!")
        logger.info(f"   Vector Store ID: {vector_store_id}")
        logger.info(f"   ã“ã®IDã‚’ä¿å­˜ã—ã¦ã€å¾Œã§RAGæ¤œç´¢ã«ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")

        # Vector Store IDã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        id_file_path = output_dir / "vector_store_id.txt"
        with open(id_file_path, 'w') as f:
            f.write(vector_store_id)
        logger.info(f"   Vector Store IDã‚’ä¿å­˜: {id_file_path}")

    else:
        logger.error("âŒ Vector Storeã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        logger.error("   ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ç¢ºèªã—ã€APIã‚­ãƒ¼ã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    # å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼
    logger.info("=== å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼ ===")
    logger.info(f"âœ… å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿: {len(df_clean)}ä»¶")
    logger.info(f"âœ… æˆåŠŸã—ãŸEmbedding: {df_clean['embedding'].notna().sum()}ä»¶")
    logger.info(f"âœ… Vector Store: {'ä½œæˆæˆåŠŸ' if vector_store_id else 'ä½œæˆå¤±æ•—'}")

    # ã‚³ã‚¹ãƒˆæ¨å®š
    total_tokens = sum(len(text.split()) * 1.3 for text in df_clean['combined_text'])  # æ¦‚ç®—
    estimated_cost = total_tokens * 0.00002 / 1000  # text-embedding-3-smallæ–™é‡‘
    logger.info(f"ğŸ’° æ¨å®šã‚³ã‚¹ãƒˆ: ç´„${estimated_cost:.4f} (æ¦‚ç®—)")

# 7-16-1: Vector Store ID: vs_68775be00d84819192ecc2b9c1039b89

# ----------------------------------------------------
# 2. Legal QA â€” *consumer_contracts_qa
# åˆ—: Question,Complex_CoT,Response
# ----------------------------------------------------

# ä¸è¶³ã—ã¦ã„ã‚‹é–¢æ•°ã‚’è¿½åŠ ï¼ˆãƒ€ãƒŸãƒ¼å®Ÿè£…ï¼‰
def standalone_search(vs_id, query):
    """æ¤œç´¢é–¢æ•°ã®ãƒ€ãƒŸãƒ¼å®Ÿè£…"""
    return f"æ¤œç´¢çµæœ: {query} (Vector Store: {vs_id})"

def extract_text_from_response(response):
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã®ãƒ€ãƒŸãƒ¼å®Ÿè£…"""
    return ["ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ã‚¹ãƒãƒ³ã‚¹"]

def create_vector_store_and_upload(text, name):
    """Vector Storeä½œæˆã®ãƒ€ãƒŸãƒ¼å®Ÿè£…"""
    return "vs_sample_id"

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("RAGãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†ãƒ—ãƒ­ã‚°ãƒ©ãƒ ")
    print("ã‚¨ãƒ©ãƒ¼ä¿®æ­£æ¸ˆã¿ç‰ˆ")

if __name__ == "__main__":
    main()
