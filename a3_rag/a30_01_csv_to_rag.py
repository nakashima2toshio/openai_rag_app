# streamlit run a30_01_csv_to_rag.py --server.port=8505
import os
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple
import time
import json

import openai
import tiktoken

import streamlit as st

# Streamlitãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="OpenAI Embedding API ã‚µãƒ³ãƒ—ãƒ«",
    page_icon="ğŸ”",
    layout="wide"
)

st.title("ğŸ” OpenAI Embedding API - FAQå‡¦ç†ã‚µãƒ³ãƒ—ãƒ«")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§APIè¨­å®š
st.sidebar.header("è¨­å®š")
api_key = os.getenv("OPENAI_API_KEY")
# api_key = st.sidebar.text_input("OpenAI API Key", type="password")
# if api_key:
#     openai.api_key = api_key

# ãƒ¢ãƒ‡ãƒ«é¸æŠ
model_options = {
    "text-embedding-3-small": "text-embedding-3-small",
    "text-embedding-3-large": "text-embedding-3-large"
}
selected_model = st.sidebar.selectbox("åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«", list(model_options.keys()))

# ãƒ‡ãƒ¼ã‚¿å½¢å¼é¸æŠ
data_format = st.sidebar.selectbox(
    "ãƒ‡ãƒ¼ã‚¿å½¢å¼",
    ["è³ªå•ã®ã¿", "å›ç­”ã®ã¿", "è³ªå•+å›ç­”çµåˆ", "å€‹åˆ¥å‡¦ç†"]
)


def count_tokens(text: str, model: str = "cl100k_base") -> int:
    """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    try:
        encoding = tiktoken.get_encoding(model)
        return len(encoding.encode(text))
    except:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ–‡å­—æ•° / 4 ã§æ¦‚ç®—
        return len(text) // 4


def preprocess_text(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†"""
    if pd.isna(text) or text is None:
        return ""

    # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    text = str(text).strip()

    # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹
    import re
    text = re.sub(r'\s+', ' ', text)

    # ç‰¹æ®Šæ–‡å­—ã®æ­£è¦åŒ–ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    text = text.replace('\u00a0', ' ')  # non-breaking space

    return text


def truncate_text(text: str, max_tokens: int = 8000) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ¶é™ã«åˆã‚ã›ã¦åˆ‡ã‚Šè©°ã‚"""
    if count_tokens(text) <= max_tokens:
        return text

    # å˜ç´”ã«æ–‡å­—æ•°ã§åˆ‡ã‚Šè©°ã‚ï¼ˆã‚ˆã‚Šæ­£ç¢ºã«ã¯ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã§åˆ‡ã‚Šè©°ã‚ã‚‹ã¹ãï¼‰
    words = text.split()
    truncated = ""
    for word in words:
        test_text = truncated + " " + word if truncated else word
        if count_tokens(test_text) > max_tokens:
            break
        truncated = test_text

    return truncated


def prepare_embedding_data(df: pd.DataFrame, format_type: str) -> List[Dict]:
    """åŸ‹ã‚è¾¼ã¿ç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"""
    data = []

    for idx, row in df.iterrows():
        question = preprocess_text(row['question'])
        answer = preprocess_text(row['answer'])

        if format_type == "è³ªå•ã®ã¿":
            if question:
                data.append({
                    'id'      : f"q_{idx}",
                    'text'    : truncate_text(question),
                    'metadata': {'type': 'question', 'index': idx}
                })

        elif format_type == "å›ç­”ã®ã¿":
            if answer:
                data.append({
                    'id'      : f"a_{idx}",
                    'text'    : truncate_text(answer),
                    'metadata': {'type': 'answer', 'index': idx}
                })

        elif format_type == "è³ªå•+å›ç­”çµåˆ":
            if question and answer:
                combined_text = f"è³ªå•: {question}\nå›ç­”: {answer}"
                data.append({
                    'id'      : f"qa_{idx}",
                    'text'    : truncate_text(combined_text),
                    'metadata': {'type': 'qa_pair', 'index': idx}
                })

        elif format_type == "å€‹åˆ¥å‡¦ç†":
            if question:
                data.append({
                    'id'      : f"q_{idx}",
                    'text'    : truncate_text(question),
                    'metadata': {'type': 'question', 'index': idx}
                })
            if answer:
                data.append({
                    'id'      : f"a_{idx}",
                    'text'    : truncate_text(answer),
                    'metadata': {'type': 'answer', 'index': idx}
                })

    return data


def get_embeddings_batch(texts: List[str], model: str, batch_size: int = 20) -> List[List[float]]:
    """ãƒãƒƒãƒã§åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—"""
    embeddings = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]

        try:
            response = openai.embeddings.create(
                input=batch,
                model=model
            )

            batch_embeddings = [data.embedding for data in response.data]
            embeddings.extend(batch_embeddings)

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            time.sleep(0.1)

        except Exception as e:
            st.error(f"åŸ‹ã‚è¾¼ã¿å–å¾—ã‚¨ãƒ©ãƒ¼ (ãƒãƒƒãƒ {i // batch_size + 1}): {str(e)}")
            return []

    return embeddings


# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
col1, col2 = st.columns([1, 1])

with col1:
    st.header("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ç¢ºèª")

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['csv'])

    if uploaded_file:
        df = pd.read_csv(uploaded_file)

        st.write("ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:")
        st.dataframe(df.head())

        st.write(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df)}")

        # ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ
        if 'question' in df.columns and 'answer' in df.columns:
            question_lengths = df['question'].fillna('').astype(str).apply(len)
            answer_lengths = df['answer'].fillna('').astype(str).apply(len)

            col1_1, col1_2 = st.columns(2)
            with col1_1:
                st.metric("è³ªå•å¹³å‡æ–‡å­—æ•°", f"{question_lengths.mean():.0f}")
                st.metric("è³ªå•æœ€å¤§æ–‡å­—æ•°", f"{question_lengths.max()}")

            with col1_2:
                st.metric("å›ç­”å¹³å‡æ–‡å­—æ•°", f"{answer_lengths.mean():.0f}")
                st.metric("å›ç­”æœ€å¤§æ–‡å­—æ•°", f"{answer_lengths.max()}")

with col2:
    st.header("ğŸš€ åŸ‹ã‚è¾¼ã¿å‡¦ç†")

    if uploaded_file and api_key:
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        embedding_data = prepare_embedding_data(df, data_format)

        st.write(f"å‡¦ç†å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆæ•°: {len(embedding_data)}")

        # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
        if embedding_data:
            st.write("å‡¦ç†å¯¾è±¡ã‚µãƒ³ãƒ—ãƒ«:")
            sample_data = embedding_data[:3]
            for i, item in enumerate(sample_data):
                with st.expander(f"ã‚µãƒ³ãƒ—ãƒ« {i + 1} (ID: {item['id']})"):
                    st.write(f"**ã‚¿ã‚¤ãƒ—:** {item['metadata']['type']}")
                    st.write(f"**ãƒˆãƒ¼ã‚¯ãƒ³æ•°:** {count_tokens(item['text'])}")
                    st.write(f"**ãƒ†ã‚­ã‚¹ãƒˆ:** {item['text'][:200]}...")

        # åŸ‹ã‚è¾¼ã¿å®Ÿè¡Œ
        if st.button("åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—é–‹å§‹", type="primary"):
            if not embedding_data:
                st.warning("å‡¦ç†å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            else:
                with st.spinner("åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—ä¸­..."):
                    texts = [item['text'] for item in embedding_data]

                    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
                    progress_bar = st.progress(0)
                    status_text = st.empty()

                    embeddings = []
                    batch_size = 20
                    total_batches = (len(texts) + batch_size - 1) // batch_size

                    for batch_idx in range(total_batches):
                        start_idx = batch_idx * batch_size
                        end_idx = min(start_idx + batch_size, len(texts))
                        batch_texts = texts[start_idx:end_idx]

                        status_text.text(f"ãƒãƒƒãƒ {batch_idx + 1}/{total_batches} å‡¦ç†ä¸­...")

                        try:
                            response = openai.embeddings.create(
                                input=batch_texts,
                                model=model_options[selected_model]
                            )

                            batch_embeddings = [data.embedding for data in response.data]
                            embeddings.extend(batch_embeddings)

                            progress_bar.progress((batch_idx + 1) / total_batches)
                            time.sleep(0.1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

                        except Exception as e:
                            st.error(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
                            break

                    if len(embeddings) == len(texts):
                        st.success(f"âœ… åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—å®Œäº†ï¼({len(embeddings)}å€‹)")

                        # çµæœã®ä¿å­˜
                        result_data = []
                        for i, (item, embedding) in enumerate(zip(embedding_data, embeddings)):
                            result_data.append({
                                'id'              : item['id'],
                                'text'            : item['text'],
                                'embedding_vector': embedding,
                                'metadata'        : item['metadata']
                            })

                        # ãƒ™ã‚¯ãƒˆãƒ«ã®çµ±è¨ˆæƒ…å ±
                        embedding_array = np.array(embeddings)
                        st.write(f"**ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒæ•°:** {embedding_array.shape[1]}")
                        st.write(f"**ãƒ™ã‚¯ãƒˆãƒ«çµ±è¨ˆ:**")
                        st.write(f"- å¹³å‡å€¤: {embedding_array.mean():.6f}")
                        st.write(f"- æ¨™æº–åå·®: {embedding_array.std():.6f}")

                        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
                        if st.button("çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆJSONï¼‰"):
                            # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¦JSONä¿å­˜å¯èƒ½ã«ã™ã‚‹
                            download_data = []
                            for item in result_data:
                                download_item = item.copy()
                                download_item['embedding_vector'] = download_item['embedding_vector']
                                download_data.append(download_item)

                            json_str = json.dumps(download_data, ensure_ascii=False, indent=2)
                            st.download_button(
                                label="ğŸ“¥ çµæœãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                data=json_str,
                                file_name=f"embeddings_{selected_model}_{data_format}.json",
                                mime="application/json"
                            )

# ä½¿ç”¨æ–¹æ³•ã®èª¬æ˜
st.header("ğŸ“ ä½¿ç”¨æ–¹æ³•ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹")

with st.expander("åŠ¹æœçš„ãªå‰å‡¦ç†ã«ã¤ã„ã¦"):
    st.markdown("""
    ### 1. ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    - ä¸è¦ãªç©ºç™½ã‚„æ”¹è¡Œã®é™¤å»
    - HTMLã‚¿ã‚°ã‚„ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®å‡¦ç†
    - ç‰¹æ®Šæ–‡å­—ã®æ­£è¦åŒ–

    ### 2. ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ¶é™
    - text-embedding-3-small/large: æœ€å¤§8,191ãƒˆãƒ¼ã‚¯ãƒ³
    - é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã¯é©åˆ‡ã«åˆ†å‰²ã¾ãŸã¯è¦ç´„

    ### 3. ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®é¸æŠ
    - **è³ªå•ã®ã¿**: è³ªå•ã«ã‚ˆã‚‹æ¤œç´¢ã«æœ€é©
    - **å›ç­”ã®ã¿**: å›ç­”å†…å®¹ã«ã‚ˆã‚‹æ¤œç´¢ã«æœ€é©  
    - **è³ªå•+å›ç­”çµåˆ**: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè±Šå¯Œã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºå¤§
    - **å€‹åˆ¥å‡¦ç†**: æŸ”è»Ÿæ€§é«˜ã€ä½†ã—2å€ã®APIã‚³ãƒ¼ãƒ«
    """)

with st.expander("ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ"):
    st.markdown("""
    ### APIä½¿ç”¨é‡å‰Šæ¸›
    - ãƒãƒƒãƒå‡¦ç†ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã‚’å‰Šæ¸›
    - é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆsmall vs largeï¼‰
    - é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®äº‹å‰é™¤å»
    - ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã®æ´»ç”¨

    ### ãƒ¢ãƒ‡ãƒ«é¸æŠæŒ‡é‡
    - **text-embedding-3-small**: ã‚³ã‚¹ãƒˆé‡è¦–ã€åŸºæœ¬çš„ãªé¡ä¼¼åº¦æ¤œç´¢
    - **text-embedding-3-large**: ç²¾åº¦é‡è¦–ã€è¤‡é›‘ãªæ„å‘³ç†è§£
    """)

# æ³¨æ„äº‹é …
st.warning(
    "âš ï¸ æ³¨æ„: æœ¬ã‚µãƒ³ãƒ—ãƒ«ã¯å­¦ç¿’ç”¨ã§ã™ã€‚æœ¬ç•ªç’°å¢ƒã§ã¯é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã€ãƒ­ã‚°æ©Ÿèƒ½ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚")