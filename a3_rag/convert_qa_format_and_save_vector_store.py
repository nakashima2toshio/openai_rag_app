# python a20_21_file_searches.py --server.port=8504
# ä¾å­˜: pip install openai pandas tqdm
import time
import json
from pathlib import Path

import pandas as pd
from openai import OpenAI
from tqdm import tqdm


# --------------------------------------------------
# datasets ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‹•çš„æ±ºå®š
# FAQã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã ã‘ã€QAå½¢å¼ã«å¤‰æ›ã™ã‚‹ã€‚
# --------------------------------------------------
def resolve_data_dir() -> Path:
    here = Path(__file__).resolve().parent
    if list(here.glob("*.csv")):               # ã‚¹ã‚¯ãƒªãƒ—ãƒˆç›´ä¸‹ã« CSV
        return here
    if (here / "datasets").exists():           # ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ datasets/
        return here / "datasets"
    raise FileNotFoundError("CSV ã¾ãŸã¯ datasets/ ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ã€‚")


DATA_DIR: Path = resolve_data_dir()
MAPPING_JSON: Path = DATA_DIR / "vector_store_mapping.json"


# --------------------------------------------------
# CSV åˆ—æŒ™
# --------------------------------------------------
def conv_qa_format(data_dir: Path = DATA_DIR) -> list[Path]:
    csv_files = sorted(data_dir.rglob("*.csv"))
    if not csv_files:
        print("âš ï¸  CSV ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:", data_dir)
    else:
        print("ğŸ“„  æ¤œå‡ºã—ãŸ CSV:")
        for p in csv_files:
            print(f"  - {p.relative_to(data_dir)}")
    return csv_files


# --------------------------------------------------
# åˆ—åæ¤œå‡ºãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# --------------------------------------------------
def detect_column(df: pd.DataFrame, candidates: set[str]) -> str | None:
    for col in df.columns:
        if col.lower().strip() in candidates:
            return col
    return None


# --------------------------------------------------
# CSV â†’ QA ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›
# --------------------------------------------------
def set_qa_format_to_dataset(csv_path: Path) -> Path | None:
    df = pd.read_csv(csv_path)

    question_aliases = {"question", "questions", "prompt", "query", "problem", "ques"}
    answer_aliases = {"answer", "answers", "response", "solution", "ans"}

    q_col = detect_column(df, question_aliases)
    a_col = detect_column(df, answer_aliases)

    if q_col is None or a_col is None:
        print(f"âš ï¸  å¿…è¦åˆ—ã‚’ç‰¹å®šã§ããšã‚¹ã‚­ãƒƒãƒ—: {csv_path.name}  cols={list(df.columns)}")
        return None

    out_path = csv_path.with_name(f"qa_{csv_path.stem}.txt")
    with out_path.open("w", encoding="utf-8") as f:
        for _, row in df.iterrows():
            q = str(row[q_col]).strip()
            a = str(row[a_col]).strip()
            f.write(f"Q: {q}\nA: {a}\n\n")

    print("ğŸ“  Plain-text saved:", out_path.relative_to(DATA_DIR))
    return out_path


# --------------------------------------------------
# Vector Store ä½œæˆ & ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# --------------------------------------------------
def create_vector_store_and_upload(txt_path: Path) -> str:
    client = OpenAI()
    vs = client.vector_stores.create(name=txt_path.name)
    vs_id = vs.id

    with txt_path.open("rb") as f:
        file_obj = client.files.create(file=f, purpose="assistants")
    client.vector_stores.files.create(vector_store_id=vs_id, file_id=file_obj.id)

    while client.vector_stores.retrieve(vs_id).status != "completed":
        time.sleep(2)

    print("âœ…  Vector Store ready:", vs_id)
    return vs_id


# --------------------------------------------------
# ãƒ¡ã‚¤ãƒ³ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
# --------------------------------------------------
def make_dataset() -> dict[str, str]:
    mapping: dict[str, str] = {}
    csv_files = conv_qa_format()

    for csv_path in tqdm(csv_files, desc="Vectorising"):
        txt_path = set_qa_format_to_dataset(csv_path)
        if txt_path is None:                   # åˆ—ä¸è¶³ã§ã‚¹ã‚­ãƒƒãƒ—
            continue
        vs_id = create_vector_store_and_upload(txt_path)
        mapping[csv_path.name] = vs_id

    if not mapping:
        print("âŒ  Vector Store ãŒ 1 ã¤ã‚‚ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        return mapping

    MAPPING_JSON.parent.mkdir(parents=True, exist_ok=True)
    MAPPING_JSON.write_text(json.dumps(mapping, ensure_ascii=False, indent=2))
    print("ğŸ’¾  Mapping saved to", MAPPING_JSON)
    return mapping


# --------------------------------------------------
# æ¤œç´¢ãƒ‡ãƒ¢ï¼ˆå˜ä¸€ VSï¼‰
# --------------------------------------------------
def standalone_search(vs_id: str, query: str = "è¿”å“ã¯ä½•æ—¥ä»¥å†…ï¼Ÿ"):
    client = OpenAI()
    results = client.vector_stores.search(vector_store_id=vs_id, query=query)
    for r in results.data:
        print(f"{r.score:.3f}", r.content[0].text.strip()[:60], "...")


# --------------------------------------------------
# æ¤œç´¢ãƒ‡ãƒ¢ï¼ˆãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ©ç”¨ï¼‰
# --------------------------------------------------
def file_searches():
    if not MAPPING_JSON.exists():
        print("âŒ  å…ˆã« make_dataset() ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return
    mapping = json.loads(MAPPING_JSON.read_text())
    for fname, vs_id in mapping.items():
        print(f"\nğŸ”  {fname}:")
        standalone_search(vs_id)


# --------------------------------------------------
# ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
# --------------------------------------------------
def main():
    make_dataset()          # Vector Store ä½œæˆ
    # file_searches()       # æ¤œç´¢ã‚’è©¦ã™å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆè§£é™¤


if __name__ == "__main__":
    main()

