# a30_013_make_rag_data_medical.py
# åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿ã®RAGå‰å‡¦ç†ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
# streamlit run a30_013_make_rag_data_medical.py --server.port=8503

import streamlit as st
import pandas as pd
from typing import List, Dict, Any
from pathlib import Path
from datetime import datetime

# å…±é€šRAGãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from helper_rag import (
        # è¨­å®šãƒ»å®šæ•°
        RAGConfig,

        # å‰å‡¦ç†é–¢æ•°ï¼ˆç§»è¡Œæ¸ˆã¿ï¼‰
        clean_text,
        combine_columns,
        additional_preprocessing,
        validate_data,

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        load_dataset,
        create_download_data,
        process_rag_data,

        # UIé–¢é€£
        setup_rag_page,
        setup_sidebar_controls,
        display_statistics,
        show_usage_instructions,
    )
except ImportError as e:
    st.error(f"helper_rag.pyã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    st.info("helper_rag.pyãŒåŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# æ—¢å­˜ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚‚ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from helper_st import UIHelper, error_handler_ui
    from helper_api import logger
except ImportError as e:
    st.warning(f"æ—¢å­˜ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ä¸€éƒ¨ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}")
    # åŸºæœ¬çš„ãªãƒ­ã‚°è¨­å®š
    import logging

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)


# ==================================================
# ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–¢æ•°
# ==================================================
def create_output_directory() -> Path:
    """OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ"""
    try:
        import os
        current_dir = os.getcwd()

        # ç›¸å¯¾ãƒ‘ã‚¹ã§OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        output_dir = Path("OUTPUT")
        absolute_output_dir = output_dir.resolve()

        logger.info(f"ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {current_dir}")
        logger.info(f"OUTPUTç›¸å¯¾ãƒ‘ã‚¹: {output_dir}")
        logger.info(f"OUTPUTçµ¶å¯¾ãƒ‘ã‚¹: {absolute_output_dir}")

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        output_dir.mkdir(exist_ok=True)

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå®Ÿéš›ã«ä½œæˆã•ã‚ŒãŸã‹ç¢ºèª
        if not output_dir.exists():
            raise OSError(f"OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã«å¤±æ•—: {absolute_output_dir}")

        # æ›¸ãè¾¼ã¿æ¨©é™ã®ãƒ†ã‚¹ãƒˆ
        test_file = output_dir / ".test_write"
        try:
            test_file.write_text("test", encoding='utf-8')
            if test_file.exists():
                test_file.unlink()
                logger.info("æ›¸ãè¾¼ã¿æ¨©é™ãƒ†ã‚¹ãƒˆ: æˆåŠŸ")
            else:
                raise PermissionError("ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—")
        except Exception as e:
            raise PermissionError(f"æ›¸ãè¾¼ã¿æ¨©é™ãƒ†ã‚¹ãƒˆã«å¤±æ•—: {e}")

        logger.info(f"OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™å®Œäº†: {absolute_output_dir}")
        return output_dir

    except PermissionError as e:
        logger.error(f"æ¨©é™ã‚¨ãƒ©ãƒ¼: {e}")
        raise PermissionError(f"OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆ/æ›¸ãè¾¼ã¿æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“: {e}")
    except Exception as e:
        logger.error(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        raise Exception(f"OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


def save_files_to_output(df_processed, dataset_type: str, csv_data: str, text_data: str = None) -> Dict[str, str]:
    """
    å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜

    Args:
        df_processed: å‡¦ç†æ¸ˆã¿DataFrame
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        csv_data: CSVãƒ‡ãƒ¼ã‚¿æ–‡å­—åˆ—
        text_data: çµåˆãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

    Returns:
        Dict[str, str]: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¾æ›¸
    """
    try:
        # ãƒ‡ãƒãƒƒã‚°: ç¾åœ¨ã®ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèª
        import os
        current_dir = os.getcwd()
        logger.info(f"ç¾åœ¨ã®ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {current_dir}")

        # OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        output_dir = create_output_directory()
        logger.info(f"OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹: {output_dir.absolute()}")

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        saved_files = {}

        # ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
        if not csv_data or len(csv_data.strip()) == 0:
            raise ValueError("CSVãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")

        logger.info(f"ä¿å­˜é–‹å§‹: {dataset_type}, {len(df_processed)}è¡Œ, ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {timestamp}")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
        csv_filename = f"preprocessed_{dataset_type}_{len(df_processed)}rows_{timestamp}.csv"
        csv_path = output_dir / csv_filename
        logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–‹å§‹: {csv_path}")

        with open(csv_path, 'w', encoding='utf-8') as f:
            f.write(csv_data)

        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå®Ÿéš›ã«ä½œæˆã•ã‚ŒãŸã‹ç¢ºèª
        if csv_path.exists():
            file_size = csv_path.stat().st_size
            saved_files['csv'] = str(csv_path)
            logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {csv_path} ({file_size} bytes)")
        else:
            raise IOError(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—: {csv_path}")

        # çµåˆãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
        if text_data and len(text_data.strip()) > 0:
            txt_filename = f"combined_{dataset_type}_{len(df_processed)}rows_{timestamp}.txt"
            txt_path = output_dir / txt_filename
            logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–‹å§‹: {txt_path}")

            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(text_data)

            if txt_path.exists():
                file_size = txt_path.stat().st_size
                saved_files['txt'] = str(txt_path)
                logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {txt_path} ({file_size} bytes)")
            else:
                logger.warning(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—: {txt_path}")
        else:
            logger.info("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—")

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
        metadata = {
            "dataset_type"        : dataset_type,
            "original_rows"       : st.session_state.get('original_rows', 0),
            "processed_rows"      : len(df_processed),
            "processing_timestamp": timestamp,
            "created_at"          : datetime.now().isoformat(),
            "working_directory"   : current_dir,
            "output_directory"    : str(output_dir.absolute()),
            "files_created"       : list(saved_files.keys())
        }

        metadata_filename = f"metadata_{dataset_type}_{timestamp}.json"
        metadata_path = output_dir / metadata_filename
        logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–‹å§‹: {metadata_path}")

        # helper_api.pyã®safe_json_dumpsã‚’ä½¿ç”¨
        try:
            from helper_api import safe_json_dumps
            metadata_json = safe_json_dumps(metadata)
        except ImportError:
            import json
            metadata_json = json.dumps(metadata, indent=2, ensure_ascii=False)

        with open(metadata_path, 'w', encoding='utf-8') as f:
            f.write(metadata_json)

        if metadata_path.exists():
            file_size = metadata_path.stat().st_size
            saved_files['metadata'] = str(metadata_path)
            logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {metadata_path} ({file_size} bytes)")
        else:
            logger.warning(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—: {metadata_path}")

        logger.info(f"å…¨ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {len(saved_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜")

        # æœ€çµ‚ç¢ºèª: ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        for file_type, file_path in saved_files.items():
            if not Path(file_path).exists():
                logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_type} - {file_path}")

        return saved_files

    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")
        import traceback
        logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")
        raise


# ==================================================
# åŒ»ç™‚QAç‰¹æœ‰ã®å‡¦ç†é–¢æ•°
# ==================================================
def validate_medical_data_specific(df) -> List[str]:
    """
    åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®æ¤œè¨¼

    Args:
        df: æ¤œè¨¼å¯¾è±¡ã®DataFrame

    Returns:
        List[str]: åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®æ¤œè¨¼çµæœ
    """
    medical_issues = []

    # åŒ»ç™‚ç”¨èªã®å­˜åœ¨ç¢ºèªï¼ˆç°¡æ˜“ç‰ˆï¼‰
    medical_keywords = [
        'ç—‡çŠ¶', 'è¨ºæ–­', 'æ²»ç™‚', 'è–¬', 'ç—…æ°—', 'ç–¾æ‚£', 'æ‚£è€…',
        'symptom', 'diagnosis', 'treatment', 'medicine', 'disease', 'patient'
    ]

    if 'Question' in df.columns:
        questions_with_medical_terms = 0
        for _, row in df.iterrows():
            question_text = str(row.get('Question', '')).lower()
            if any(keyword in question_text for keyword in medical_keywords):
                questions_with_medical_terms += 1

        medical_ratio = (questions_with_medical_terms / len(df)) * 100
        medical_issues.append(f"åŒ»ç™‚é–¢é€£ç”¨èªã‚’å«ã‚€è³ªå•: {questions_with_medical_terms}ä»¶ ({medical_ratio:.1f}%)")

    # å›ç­”ã®é•·ã•åˆ†æï¼ˆåŒ»ç™‚å›ç­”ã¯é€šå¸¸è©³ç´°ï¼‰
    if 'Response' in df.columns:
        response_lengths = df['Response'].astype(str).str.len()
        avg_response_length = response_lengths.mean()
        if avg_response_length < 100:
            medical_issues.append(f"âš ï¸ å¹³å‡å›ç­”é•·ãŒçŸ­ã„å¯èƒ½æ€§: {avg_response_length:.0f}æ–‡å­—")
        else:
            medical_issues.append(f"âœ… é©åˆ‡ãªå›ç­”é•·: å¹³å‡{avg_response_length:.0f}æ–‡å­—")

    return medical_issues


def display_medical_specific_info():
    """åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®æƒ…å ±ã‚’è¡¨ç¤º"""
    with st.sidebar.expander("ğŸ¥ åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰è¨­å®š", expanded=False):
        st.write("**åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´:**")
        st.write("- å°‚é–€ç”¨èªã®å¤šç”¨")
        st.write("- è©³ç´°ãªèª¬æ˜æ–‡")
        st.write("- æ­£ç¢ºæ€§ãŒé‡è¦")

        st.write("**å‰å‡¦ç†ã®æ³¨æ„ç‚¹:**")
        st.write("- åŒ»ç™‚ç”¨èªã®ä¿æŒ")
        st.write("- æ–‡è„ˆã®ä¿æŒ")
        st.write("- ç•¥èªã®å±•é–‹")

        # åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        preserve_medical_terms = st.checkbox(
            "åŒ»ç™‚ç”¨èªã‚’ä¿è­·",
            value=True,
            help="åŒ»ç™‚å°‚é–€ç”¨èªã®éåº¦ãªæ­£è¦åŒ–ã‚’é˜²ã"
        )

        expand_abbreviations = st.checkbox(
            "ç•¥èªã‚’å±•é–‹",
            value=False,
            help="ä¸€èˆ¬çš„ãªåŒ»ç™‚ç•¥èªã‚’å±•é–‹å½¢ã«å¤‰æ›"
        )

        return preserve_medical_terms, expand_abbreviations


# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•°
# ==================================================
@error_handler_ui
def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•°"""

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã®è¨­å®š
    DATASET_TYPE = "medical_qa"

    # ãƒšãƒ¼ã‚¸è¨­å®šï¼ˆç‹¬ç«‹å®Ÿè¡Œï¼‰
    try:
        st.set_page_config(
            page_title="åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†",
            page_icon="ğŸ¥",
            layout="wide"
        )
    except st.errors.StreamlitAPIException:
        # æ—¢ã«è¨­å®šæ¸ˆã¿ã®å ´åˆã¯ç„¡è¦–
        pass

    st.title("ğŸ¥ åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚¢ãƒ—ãƒª")
    st.markdown("---")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®šï¼ˆç‹¬ç«‹å®Ÿè¡Œï¼‰
    st.sidebar.header("è¨­å®š")
    combine_columns_option = st.sidebar.checkbox(
        "è¤‡æ•°åˆ—ã‚’çµåˆã™ã‚‹ï¼ˆVector Storeç”¨ï¼‰",
        value=True,
        help="è¤‡æ•°åˆ—ã‚’çµåˆã—ã¦RAGç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ"
    )
    show_validation = st.sidebar.checkbox(
        "ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚’è¡¨ç¤º",
        value=True,
        help="ãƒ‡ãƒ¼ã‚¿ã®å“è³ªæ¤œè¨¼çµæœã‚’è¡¨ç¤º"
    )

    # åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®è¨­å®šï¼ˆç°¡ç•¥åŒ–ï¼‰
    with st.sidebar.expander("ğŸ¥ åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿è¨­å®š", expanded=False):
        preserve_medical_terms = st.checkbox(
            "åŒ»ç™‚ç”¨èªã‚’ä¿è­·",
            value=True,
            help="åŒ»ç™‚å°‚é–€ç”¨èªã®éåº¦ãªæ­£è¦åŒ–ã‚’é˜²ã"
        )
        expand_abbreviations = st.checkbox(
            "ç•¥èªã‚’å±•é–‹",
            value=False,
            help="ä¸€èˆ¬çš„ãªåŒ»ç™‚ç•¥èªã‚’å±•é–‹å½¢ã«å¤‰æ›"
        )

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    st.subheader("ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_file = st.file_uploader(
        "åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
        type=['csv'],
        help="Question, Complex_CoT, Response ã®3åˆ—ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«",
        key="medical_qa_uploader"
    )
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    st.subheader("ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_file = st.file_uploader(
        "åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
        type=['csv'],
        help="Question, Complex_CoT, Response ã®3åˆ—ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«"
    )

    if uploaded_file is not None:
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®ç¢ºèª
            st.info(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {uploaded_file.name} ({uploaded_file.size:,} bytes)")

            # DataFrameã®èª­ã¿è¾¼ã¿
            df = pd.read_csv(uploaded_file)

            # åŸºæœ¬æ¤œè¨¼
            validation_results = validate_data(df, DATASET_TYPE)

            logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ, {len(df.columns)}åˆ—")
            st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸã€‚è¡Œæ•°: {len(df)}")

            # å…ƒãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
            st.subheader("ğŸ“‹ å…ƒãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            st.dataframe(df.head(10))

            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼çµæœã®è¡¨ç¤º
            if show_validation:
                st.subheader("ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼")
                for issue in validation_results:
                    st.info(issue)

                # åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®æ¤œè¨¼
                medical_issues = validate_medical_data_specific(df)
                if medical_issues:
                    st.write("**åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®åˆ†æ:**")
                    for issue in medical_issues:
                        st.info(issue)

            # å‰å‡¦ç†å®Ÿè¡Œ
            st.subheader("âš™ï¸ å‰å‡¦ç†å®Ÿè¡Œ")

            if st.button("å‰å‡¦ç†ã‚’å®Ÿè¡Œ", type="primary"):
                try:
                    with st.spinner("å‰å‡¦ç†ä¸­..."):
                        # RAGãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
                        df_processed = process_rag_data(
                            df,
                            DATASET_TYPE,
                            combine_columns_option
                        )

                    st.success("å‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

                    # å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
                    st.subheader("âœ… å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                    st.dataframe(df_processed.head(10))

                    # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
                    display_statistics(df, df_processed, DATASET_TYPE)

                    # åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®å¾Œå‡¦ç†åˆ†æ
                    if 'Combined_Text' in df_processed.columns:
                        st.subheader("ğŸ¥ åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®åˆ†æ")

                        # çµåˆãƒ†ã‚­ã‚¹ãƒˆã®åŒ»ç™‚ç”¨èªåˆ†æ
                        combined_texts = df_processed['Combined_Text']
                        medical_keywords = ['ç—‡çŠ¶', 'è¨ºæ–­', 'æ²»ç™‚', 'è–¬', 'ç—…æ°—', 'ç–¾æ‚£']

                        keyword_counts = {}
                        for keyword in medical_keywords:
                            count = combined_texts.str.contains(keyword, case=False).sum()
                            keyword_counts[keyword] = count

                        if keyword_counts:
                            st.write("**åŒ»ç™‚ç”¨èªã®å‡ºç¾é »åº¦:**")
                            for keyword, count in keyword_counts.items():
                                percentage = (count / len(df_processed)) * 100
                                st.write(f"- {keyword}: {count}ä»¶ ({percentage:.1f}%)")

                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ä¿å­˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³
                    st.subheader("ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ä¿å­˜")

                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
                    csv_data, text_data = create_download_data(
                        df_processed,
                        combine_columns_option,
                        DATASET_TYPE
                    )

                    # ãƒ–ãƒ©ã‚¦ã‚¶ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                    st.write("**ğŸ“¥ ãƒ–ãƒ©ã‚¦ã‚¶ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**")
                    col1, col2 = st.columns(2)

                    with col1:
                        st.download_button(
                            label="ğŸ“Š CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=csv_data,
                            file_name=f"preprocessed_{DATASET_TYPE}_{len(df_processed)}rows.csv",
                            mime="text/csv",
                            help="å‰å‡¦ç†æ¸ˆã¿ã®åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿ã‚’CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
                        )

                    with col2:
                        if text_data:
                            st.download_button(
                                label="ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                data=text_data,
                                file_name=f"combined_{DATASET_TYPE}_{len(df_processed)}rows.txt",
                                mime="text/plain",
                                help="Vector Store/RAGç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸçµåˆãƒ†ã‚­ã‚¹ãƒˆ"
                            )

                    # ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜
                    st.write("**ğŸ’¾ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼ˆOUTPUTãƒ•ã‚©ãƒ«ãƒ€ï¼‰**")

                    if st.button("ğŸ”„ OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜", type="secondary"):
                        try:
                            with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ä¸­..."):
                                saved_files = save_files_to_output(
                                    df_processed,
                                    DATASET_TYPE,
                                    csv_data,
                                    text_data
                                )

                            if saved_files:
                                st.success("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†ï¼")

                                # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
                                with st.expander("ğŸ“‚ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§", expanded=True):
                                    for file_type, file_path in saved_files.items():
                                        if Path(file_path).exists():
                                            file_size = Path(file_path).stat().st_size
                                            st.write(f"**{file_type.upper()}**: `{file_path}` ({file_size:,} bytes) âœ…")
                                        else:
                                            st.write(
                                                f"**{file_type.upper()}**: `{file_path}` âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

                                    # OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã®å ´æ‰€ã‚’è¡¨ç¤º
                                    output_path = Path("OUTPUT").resolve()
                                    st.write(f"**ä¿å­˜å ´æ‰€**: `{output_path}`")
                                    file_count = len(list(output_path.glob("*")))
                                    st.write(f"**ãƒ•ã‚©ãƒ«ãƒ€å†…ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {file_count}å€‹")
                            else:
                                st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")

                        except Exception as save_error:
                            st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(save_error)}")
                            logger.error(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {save_error}")

                            with st.expander("ğŸ”§ ä¿å­˜ã‚¨ãƒ©ãƒ¼è©³ç´°", expanded=False):
                                import traceback
                                st.code(traceback.format_exc())

                    # æ—¢å­˜ã®ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
                    try:
                        output_dir = Path("OUTPUT")
                        if output_dir.exists():
                            saved_files_list = list(output_dir.glob(f"*{DATASET_TYPE}*"))
                            if saved_files_list:
                                st.write("**ğŸ“ æ—¢å­˜ã®ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«**")
                                with st.expander("ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§", expanded=False):
                                    for file_path in sorted(saved_files_list, reverse=True)[:5]:  # æœ€æ–°5ä»¶
                                        file_stats = file_path.stat()
                                        file_time = datetime.fromtimestamp(file_stats.st_mtime).strftime(
                                            "%Y-%m-%d %H:%M:%S")
                                        st.write(f"- `{file_path.name}` ({file_stats.st_size:,} bytes, {file_time})")
                    except Exception as list_error:
                        logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {list_error}")

                    logger.info(f"åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†: {len(df)} â†’ {len(df_processed)}è¡Œ")

                except Exception as process_error:
                    st.error(f"å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(process_error)}")
                    logger.error(f"å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {process_error}")

                    with st.expander("ğŸ”§ å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼è©³ç´°", expanded=False):
                        import traceback
                        st.code(traceback.format_exc())

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

            # è©³ç´°ãªã‚¨ãƒ©ãƒ¼è¨ºæ–­
            with st.expander("ğŸ”§ è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±", expanded=True):
                import traceback
                st.code(traceback.format_exc())

                # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®è©³ç´°ç¢ºèª
                st.write("**ãƒ•ã‚¡ã‚¤ãƒ«è¨ºæ–­:**")
                if uploaded_file is not None:
                    st.write(f"- ãƒ•ã‚¡ã‚¤ãƒ«å: {uploaded_file.name}")
                    st.write(f"- ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {uploaded_file.size:,} bytes")
                    st.write(f"- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: {uploaded_file.type}")
                    st.write(f"- ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå‹: {type(uploaded_file)}")

                    # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®ãƒ†ã‚¹ãƒˆèª­ã¿è¾¼ã¿
                    try:
                        uploaded_file.seek(0)  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ
                        sample_content = uploaded_file.read(200).decode('utf-8', errors='ignore')
                        st.write("**ãƒ•ã‚¡ã‚¤ãƒ«å…ˆé ­ï¼ˆ200æ–‡å­—ï¼‰:**")
                        st.code(sample_content)
                        uploaded_file.seek(0)  # ãƒªã‚»ãƒƒãƒˆ
                    except Exception as read_error:
                        st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {read_error}")
                else:
                    st.write("- ãƒ•ã‚¡ã‚¤ãƒ«: None")

                # ç’°å¢ƒæƒ…å ±
                st.write("**ç’°å¢ƒæƒ…å ±:**")
                import pandas as pd
                st.write(f"- pandas version: {pd.__version__}")
                st.write(f"- streamlit version: {st.__version__}")

    else:
        st.info("ğŸ‘† CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã¨åŸºæœ¬æ¤œè¨¼
            df, validation_results = load_dataset(uploaded_file, DATASET_TYPE)

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®éš›ã«å…ƒã®è¡Œæ•°ã‚’ä¿å­˜
            st.session_state['original_rows'] = len(df)
            st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸã€‚è¡Œæ•°: {len(df)}")

            # å…ƒãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
            st.subheader("ğŸ“‹ å…ƒãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            st.dataframe(df.head(10))

            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼çµæœã®è¡¨ç¤º
            if show_validation:
                st.subheader("ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼")

                # åŸºæœ¬æ¤œè¨¼çµæœ
                for issue in validation_results:
                    st.info(issue)

                # åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®æ¤œè¨¼
                medical_issues = validate_medical_data_specific(df)
                if medical_issues:
                    st.write("**åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®åˆ†æ:**")
                    for issue in medical_issues:
                        st.info(issue)

            # å‰å‡¦ç†å®Ÿè¡Œ
            st.subheader("âš™ï¸ å‰å‡¦ç†å®Ÿè¡Œ")

            if st.button("å‰å‡¦ç†ã‚’å®Ÿè¡Œ", type="primary"):
                # RAGãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
                df_processed = process_rag_data(
                    df,
                    DATASET_TYPE,
                    combine_columns_option
                )

                st.success("å‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

                # å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
                st.subheader("âœ… å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                st.dataframe(df_processed.head(10))

                # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
                display_statistics(df, df_processed, DATASET_TYPE)

                # åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®å¾Œå‡¦ç†åˆ†æ
                if 'Combined_Text' in df_processed.columns:
                    st.subheader("ğŸ¥ åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®åˆ†æ")

                    # çµåˆãƒ†ã‚­ã‚¹ãƒˆã®åŒ»ç™‚ç”¨èªåˆ†æ
                    combined_texts = df_processed['Combined_Text']
                    medical_keywords = ['ç—‡çŠ¶', 'è¨ºæ–­', 'æ²»ç™‚', 'è–¬', 'ç—…æ°—', 'ç–¾æ‚£']

                    keyword_counts = {}
                    for keyword in medical_keywords:
                        count = combined_texts.str.contains(keyword, case=False).sum()
                        keyword_counts[keyword] = count

                    if keyword_counts:
                        st.write("**åŒ»ç™‚ç”¨èªã®å‡ºç¾é »åº¦:**")
                        for keyword, count in keyword_counts.items():
                            percentage = (count / len(df_processed)) * 100
                            st.write(f"- {keyword}: {count}ä»¶ ({percentage:.1f}%)")

                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
                st.subheader("ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ä¿å­˜")

                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
                csv_data, text_data = create_download_data(
                    df_processed,
                    combine_columns_option,
                    DATASET_TYPE
                )

                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶çµŒç”±ï¼‰
                st.write("**ğŸ“¥ ãƒ–ãƒ©ã‚¦ã‚¶ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**")
                col1, col2 = st.columns(2)

                with col1:
                    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                    st.download_button(
                        label="ğŸ“Š CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=csv_data,
                        file_name=f"preprocessed_{DATASET_TYPE}_{str(len(df_processed))}rows.csv",
                        mime="text/csv",
                        help="å‰å‡¦ç†æ¸ˆã¿ã®åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿ã‚’CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
                    )

                with col2:
                    # çµåˆãƒ†ã‚­ã‚¹ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                    if text_data:
                        st.download_button(
                            label="ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=text_data,
                            file_name=f"combined_{DATASET_TYPE}_{str(len(df_processed))}rows.txt",
                            mime="text/plain",
                            help="Vector Store/RAGç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸçµåˆãƒ†ã‚­ã‚¹ãƒˆ"
                        )

                # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜æ©Ÿèƒ½ï¼ˆOUTPUTãƒ•ã‚©ãƒ«ãƒ€ï¼‰
                st.write("**ğŸ’¾ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼ˆOUTPUTãƒ•ã‚©ãƒ«ãƒ€ï¼‰**")

                col1, col2 = st.columns([2, 1])
                with col1:
                    st.info("ğŸ’¡ ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã•ã‚Œã¾ã™")

                with col2:
                    if st.button("ğŸ”„ OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜", type="primary"):
                        # ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯ç¢ºèª
                        st.write("ğŸ”„ ä¿å­˜å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
                        logger.info("=== ä¿å­˜ãƒœã‚¿ãƒ³ãŒã‚¯ãƒªãƒƒã‚¯ã•ã‚Œã¾ã—ãŸ ===")

                        try:
                            # ãƒ‡ãƒãƒƒã‚°: ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª
                            logger.info(f"csv_dataå­˜åœ¨ç¢ºèª: {csv_data is not None}")
                            logger.info(f"csv_dataé•·ã•: {len(csv_data) if csv_data else 0}")
                            logger.info(f"text_dataå­˜åœ¨ç¢ºèª: {text_data is not None}")
                            logger.info(f"text_dataé•·ã•: {len(text_data) if text_data else 0}")
                            logger.info(f"df_processedè¡Œæ•°: {len(df_processed)}")

                            with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ä¸­..."):
                                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤º
                                logger.info("ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å‡¦ç†é–‹å§‹")

                                # ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèªï¼ˆè©³ç´°ï¼‰
                                if not csv_data:
                                    error_msg = "CSVãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™"
                                    st.error(f"âŒ {error_msg}")
                                    logger.error(error_msg)

                                    # ãƒ‡ãƒ¼ã‚¿å†ä½œæˆã‚’è©¦è¡Œ
                                    st.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã®å†ä½œæˆã‚’è©¦è¡Œä¸­...")
                                    try:
                                        logger.info("csv_dataå†ä½œæˆé–‹å§‹")
                                        csv_data, text_data = create_download_data(
                                            df_processed,
                                            combine_columns_option,
                                            DATASET_TYPE
                                        )
                                        logger.info(
                                            f"å†ä½œæˆå¾Œ - csv_data: {len(csv_data)}, text_data: {len(text_data) if text_data else 0}")

                                        if not csv_data:
                                            st.error("âŒ ãƒ‡ãƒ¼ã‚¿ã®å†ä½œæˆã«ã‚‚å¤±æ•—ã—ã¾ã—ãŸ")
                                            logger.error("ãƒ‡ãƒ¼ã‚¿å†ä½œæˆå¤±æ•—")
                                            return
                                        else:
                                            st.success("âœ… ãƒ‡ãƒ¼ã‚¿ã®å†ä½œæˆã«æˆåŠŸ")
                                            logger.info("ãƒ‡ãƒ¼ã‚¿å†ä½œæˆæˆåŠŸ")
                                    except Exception as recreate_error:
                                        st.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å†ä½œæˆã‚¨ãƒ©ãƒ¼: {recreate_error}")
                                        logger.error(f"ãƒ‡ãƒ¼ã‚¿å†ä½œæˆã‚¨ãƒ©ãƒ¼: {recreate_error}")
                                        return

                                # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã®è©³ç´°ç¢ºèª
                                st.info(
                                    f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºç¢ºèª: CSV={len(csv_data):,}æ–‡å­—, TXT={len(text_data) if text_data else 0:,}æ–‡å­—")

                                # ä¿å­˜å‡¦ç†ã®å®Ÿè¡Œ
                                logger.info("save_files_to_outputé–¢æ•°å‘¼ã³å‡ºã—é–‹å§‹")
                                saved_files = save_files_to_output(
                                    df_processed,
                                    DATASET_TYPE,
                                    csv_data,
                                    text_data
                                )
                                logger.info(f"save_files_to_outputé–¢æ•°å®Œäº†: {saved_files}")

                                if saved_files:
                                    st.success("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†ï¼")
                                    logger.info(f"ä¿å­˜æˆåŠŸ: {len(saved_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")

                                    # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
                                    with st.expander("ğŸ“‚ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§", expanded=True):
                                        for file_type, file_path in saved_files.items():
                                            try:
                                                if Path(file_path).exists():
                                                    file_size = Path(file_path).stat().st_size
                                                    st.write(
                                                        f"**{file_type.upper()}**: `{file_path}` ({file_size:,} bytes) âœ…")
                                                    logger.info(
                                                        f"ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªOK: {file_type} - {file_path} ({file_size} bytes)")
                                                else:
                                                    st.write(
                                                        f"**{file_type.upper()}**: `{file_path}` âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                                                    logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªNG: {file_type} - {file_path}")
                                            except Exception as e:
                                                st.write(f"**{file_type.upper()}**: `{file_path}` âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                                                logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªã‚¨ãƒ©ãƒ¼: {file_type} - {e}")

                                        # OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã®å ´æ‰€ã‚’è¡¨ç¤º
                                        try:
                                            output_path = Path("OUTPUT").resolve()
                                            st.write(f"**ä¿å­˜å ´æ‰€**: `{output_path}`")

                                            # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°
                                            file_count = len(list(output_path.glob("*")))
                                            st.write(f"**ãƒ•ã‚©ãƒ«ãƒ€å†…ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {file_count}å€‹")
                                            logger.info(f"OUTPUTãƒ•ã‚©ãƒ«ãƒ€: {output_path}, ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {file_count}")
                                        except Exception as e:
                                            st.error(f"ãƒ‘ã‚¹æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                                            logger.error(f"ãƒ‘ã‚¹æƒ…å ±ã‚¨ãƒ©ãƒ¼: {e}")

                                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®æ›´æ–°
                                    st.session_state['files_saved'] = True
                                    st.session_state['last_save_time'] = datetime.now().isoformat()
                                    logger.info("ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹æ›´æ–°å®Œäº†")
                                else:
                                    st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆæˆ»ã‚Šå€¤ãŒç©ºï¼‰")
                                    logger.error("ä¿å­˜é–¢æ•°ã‹ã‚‰ç©ºã®æˆ»ã‚Šå€¤")

                        except Exception as e:
                            st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
                            logger.error(f"ä¿å­˜å‡¦ç†ç·åˆã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")

                            # ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã®è©³ç´°ãƒ­ã‚°
                            import traceback
                            logger.error(f"ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯: {traceback.format_exc()}")

                            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤º
                            with st.expander("ğŸ”§ ãƒ‡ãƒãƒƒã‚°æƒ…å ±", expanded=True):
                                st.code(traceback.format_exc())

                                # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæƒ…å ±
                                try:
                                    import os
                                    current_dir = os.getcwd()
                                    st.write(f"ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {current_dir}")

                                    # OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã®çŠ¶æ…‹ç¢ºèª
                                    output_path = Path("OUTPUT")
                                    if output_path.exists():
                                        st.write(f"OUTPUTãƒ•ã‚©ãƒ«ãƒ€: å­˜åœ¨ ({output_path.resolve()})")
                                        try:
                                            files = list(output_path.glob("*"))
                                            st.write(f"ãƒ•ã‚©ãƒ«ãƒ€å†…ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)}")
                                            if files:
                                                st.write("ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:")
                                                for f in files[:5]:  # æœ€åˆã®5å€‹ã‚’è¡¨ç¤º
                                                    st.write(f"  - {f.name}")
                                        except Exception as file_error:
                                            st.write(f"ãƒ•ã‚©ãƒ«ãƒ€å†…å®¹ã®ç¢ºèªã«å¤±æ•—: {file_error}")
                                    else:
                                        st.write("OUTPUTãƒ•ã‚©ãƒ«ãƒ€: å­˜åœ¨ã—ãªã„")

                                    # å¤‰æ•°ã®çŠ¶æ…‹ç¢ºèª
                                    st.write("**å¤‰æ•°ã®çŠ¶æ…‹:**")
                                    st.write(f"- csv_data: {type(csv_data)} (é•·ã•: {len(csv_data) if csv_data else 0})")
                                    st.write(
                                        f"- text_data: {type(text_data)} (é•·ã•: {len(text_data) if text_data else 0})")
                                    st.write(f"- df_processed: {type(df_processed)} (è¡Œæ•°: {len(df_processed)})")
                                    st.write(f"- DATASET_TYPE: {DATASET_TYPE}")

                                except Exception as debug_e:
                                    st.write(f"ãƒ‡ãƒãƒƒã‚°æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {debug_e}")
                                    logger.error(f"ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚¨ãƒ©ãƒ¼: {debug_e}")

                            # æ‰‹å‹•ã§ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆã‚’ææ¡ˆ
                            st.info("ğŸ’¡ æ‰‹å‹•ã§OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„")

                            if st.button("ğŸ”„ å†è©¦è¡Œ", key="retry_save"):
                                logger.info("å†è©¦è¡Œãƒœã‚¿ãƒ³ãŒã‚¯ãƒªãƒƒã‚¯ã•ã‚Œã¾ã—ãŸ")
                                st.rerun()

                # æ—¢å­˜ã®ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
                st.write("**ğŸ“ æ—¢å­˜ã®ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«**")
                try:
                    output_dir = Path("OUTPUT")
                    if output_dir.exists() and output_dir.is_dir():
                        saved_files_list = list(output_dir.glob(f"*{DATASET_TYPE}*"))
                        if saved_files_list:
                            with st.expander("ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§", expanded=False):
                                for file_path in sorted(saved_files_list, reverse=True)[:10]:  # æœ€æ–°10ä»¶
                                    try:
                                        file_stats = file_path.stat()
                                        file_time = datetime.fromtimestamp(file_stats.st_mtime).strftime(
                                            "%Y-%m-%d %H:%M:%S")
                                        st.write(f"- `{file_path.name}` ({file_stats.st_size:,} bytes, {file_time})")
                                    except Exception as e:
                                        st.write(f"- `{file_path.name}` (æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e})")

                                if len(saved_files_list) > 10:
                                    st.write(f"... ä»– {len(saved_files_list) - 10} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")
                        else:
                            st.info(f"ğŸ“‚ OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã«{DATASET_TYPE}é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    else:
                        st.info("ğŸ“‚ OUTPUTãƒ•ã‚©ãƒ«ãƒ€ãŒã¾ã ä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")

                        # ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆãƒœã‚¿ãƒ³ã‚’æä¾›
                        if st.button("ğŸ“ OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ", key="create_output_dir"):
                            try:
                                create_output_directory()
                                st.success("âœ… OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¾ã—ãŸ")
                                st.rerun()
                            except Exception as e:
                                st.error(f"âŒ ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
                except Exception as e:
                    st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                    logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚¨ãƒ©ãƒ¼: {e}")

                # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆï¼‰
                st.session_state[f'processed_{DATASET_TYPE}'] = {
                    'data'                  : df_processed,
                    'original_rows'         : st.session_state.get('original_rows', len(df)),
                    'processed_rows'        : len(df_processed),
                    'timestamp'             : datetime.now().isoformat(),
                    'files_saved'           : False,  # ã¾ã ä¿å­˜ã•ã‚Œã¦ã„ãªã„
                    'csv_data_size'         : len(csv_data),
                    'text_data_size'        : len(text_data) if text_data else 0,
                    # é‡è¦ï¼šä¿å­˜ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                    'csv_data'              : csv_data,
                    'text_data'             : text_data,
                    'combine_columns_option': combine_columns_option
                }

                logger.info(f"åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†: {len(df)} â†’ {len(df_processed)}è¡Œ")

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            logger.error(f"åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            if st.checkbox("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¡¨ç¤º"):
                st.exception(e)

    # ä½¿ç”¨æ–¹æ³•ã®èª¬æ˜
    show_usage_instructions(DATASET_TYPE)

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    if st.sidebar.checkbox("ğŸ”§ ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’è¡¨ç¤º", value=False):
        with st.sidebar.expander("ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹", expanded=False):
            if f'processed_{DATASET_TYPE}' in st.session_state:
                processed_info = st.session_state[f'processed_{DATASET_TYPE}']
                st.write(f"**å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿**: {processed_info['processed_rows']}è¡Œ")
                st.write(f"**å…ƒãƒ‡ãƒ¼ã‚¿**: {processed_info['original_rows']}è¡Œ")
                st.write(f"**å‡¦ç†æ™‚åˆ»**: {processed_info.get('timestamp', 'N/A')}")

                # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜çŠ¶æ…‹
                if 'files_saved' in processed_info:
                    if processed_info['files_saved']:
                        st.write("**ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜**: âœ… å®Œäº†")
                        if 'last_save_time' in st.session_state:
                            st.write(f"**æœ€çµ‚ä¿å­˜æ™‚åˆ»**: {st.session_state['last_save_time']}")
                    else:
                        st.write("**ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜**: âŒ æœªä¿å­˜")

                # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºæƒ…å ±
                if 'csv_data_size' in processed_info:
                    st.write(f"**CSVãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º**: {processed_info['csv_data_size']:,} æ–‡å­—")
                if 'text_data_size' in processed_info:
                    st.write(f"**ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º**: {processed_info['text_data_size']:,} æ–‡å­—")

                # OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã®çŠ¶æ…‹
                try:
                    output_dir = Path("OUTPUT")
                    if output_dir.exists():
                        file_count = len(list(output_dir.glob(f"*{DATASET_TYPE}*")))
                        st.write(f"**ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«**: {file_count}å€‹")
                        st.write(f"**ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹**: `{output_dir.resolve()}`")
                    else:
                        st.write("**OUTPUTãƒ•ã‚©ãƒ«ãƒ€**: æœªä½œæˆ")
                except Exception as e:
                    st.write(f"**ãƒ•ã‚©ãƒ«ãƒ€çŠ¶æ…‹**: ã‚¨ãƒ©ãƒ¼ ({e})")
            else:
                st.write("å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãªã—")


# ==================================================
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
# ==================================================
if __name__ == "__main__":
    main()

# å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰:
# streamlit run a30_013_make_rag_data_medical.py --server.port=8503
