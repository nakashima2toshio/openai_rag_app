# helper_rag.py
# RAGãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®å…±é€šæ©Ÿèƒ½
# -----------------------------------------
import re
import io
import logging
from typing import List, Dict, Any, Optional, Union, Tuple
from datetime import datetime
from pathlib import Path

import pandas as pd
import streamlit as st

# æ—¢å­˜ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from helper_api import (
        config, logger, safe_json_dumps,
        format_timestamp, sanitize_key
    )
    from helper_st import (
        UIHelper, SessionStateManager, error_handler_ui
    )
except ImportError as e:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ãƒ­ã‚°è¨­å®š
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    logger.warning(f"ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


# ==================================================
# RAGãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®å…±é€šè¨­å®š
# ==================================================
class RAGConfig:
    """RAGå‰å‡¦ç†ã®è¨­å®šç®¡ç†"""

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ã®è¨­å®š
    DATASET_CONFIGS = {
        "medical_qa"          : {
            "name"            : "åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿",
            "icon"            : "ğŸ¥",
            "required_columns": ["Question", "Complex_CoT", "Response"],
            "description"     : "åŒ»ç™‚è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {complex_cot} {response}"
        },
        "customer_support_faq": {
            "name"            : "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ»FAQ",
            "icon"            : "ğŸ’¬",
            "required_columns": ["question", "answer"],  # è¦ç¢ºèª
            "description"     : "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {answer}"
        },
        "legal_qa"            : {
            "name"            : "æ³•å¾‹ãƒ»åˆ¤ä¾‹QA",
            "icon"            : "âš–ï¸",
            "required_columns": ["question", "answer"],  # è¦ç¢ºèª
            "description"     : "æ³•å¾‹ãƒ»åˆ¤ä¾‹è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {answer}"
        },
        "sciq_qa"             : {
            "name"            : "ç§‘å­¦ãƒ»æŠ€è¡“QA",
            "icon"            : "ğŸ”¬",
            "required_columns": ["question", "correct_answer"],  # è¦ç¢ºèª
            "description"     : "ç§‘å­¦ãƒ»æŠ€è¡“è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {correct_answer}"
        },
        "trivia_qa"           : {
            "name"            : "ä¸€èˆ¬çŸ¥è­˜ãƒ»ãƒˆãƒªãƒ“ã‚¢QA",
            "icon"            : "ğŸ§ ",
            "required_columns": ["question", "answer"],  # è¦ç¢ºèª
            "description"     : "ä¸€èˆ¬çŸ¥è­˜ãƒ»ãƒˆãƒªãƒ“ã‚¢è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {answer}"
        }
    }

    @classmethod
    def get_config(cls, dataset_type: str) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã®å–å¾—"""
        return cls.DATASET_CONFIGS.get(dataset_type, {
            "name"            : "æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "icon"            : "â“",
            "required_columns": [],
            "description"     : "æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{}"
        })


# ==================================================
# ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã®å…±é€šé–¢æ•°
# ==================================================
def clean_text(text: str) -> str:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†
    - æ”¹è¡Œã®é™¤å»
    - é€£ç¶šã—ãŸç©ºç™½ã‚’1ã¤ã®ç©ºç™½ã«ã¾ã¨ã‚ã‚‹
    - ä¸è¦ãªæ–‡å­—ã®æ­£è¦åŒ–

    Args:
        text (str): å‡¦ç†å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        str: ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ
    """
    if pd.isna(text) or text == "":
        return ""

    # æ”¹è¡Œã‚’ç©ºç™½ã«ç½®æ›
    text = text.replace('\n', ' ').replace('\r', ' ')

    # é€£ç¶šã—ãŸç©ºç™½ã‚’1ã¤ã®ç©ºç™½ã«ã¾ã¨ã‚ã‚‹
    text = re.sub(r'\s+', ' ', text)

    # å…ˆé ­ãƒ»æœ«å°¾ã®ç©ºç™½ã‚’é™¤å»
    text = text.strip()

    # å¼•ç”¨ç¬¦ã®æ­£è¦åŒ–
    text = text.replace('"', '"').replace('"', '"')
    text = text.replace(''', "'").replace(''', "'")

    return text


def combine_columns(
        row: pd.Series,
        dataset_type: str = "medical_qa",
        custom_template: str = None
) -> str:
    """
    è¤‡æ•°åˆ—ã‚’çµåˆã—ã¦1ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã«ã™ã‚‹ï¼ˆVector Store/RAGç”¨ã«æœ€é©åŒ–ï¼‰

    Args:
        row (pd.Series): DataFrameã®1è¡Œãƒ‡ãƒ¼ã‚¿
        dataset_type (str): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        custom_template (str, optional): ã‚«ã‚¹ã‚¿ãƒ çµåˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

    Returns:
        str: çµåˆæ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ
    """
    config_data = RAGConfig.get_config(dataset_type)
    required_columns = config_data["required_columns"]
    template = custom_template or config_data["combine_template"]

    # å„åˆ—ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºãƒ»ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    cleaned_values = {}
    for col in required_columns:
        value = row.get(col, '')
        cleaned_values[col.lower()] = clean_text(str(value))

    try:
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦çµåˆ
        if dataset_type == "medical_qa":
            # åŒ»ç™‚QAç”¨ã®ç‰¹åˆ¥å‡¦ç†
            question = cleaned_values.get('question', '')
            complex_cot = cleaned_values.get('complex_cot', '')
            response = cleaned_values.get('response', '')
            combined = f"{question} {complex_cot} {response}"
        else:
            # ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã®æ±ç”¨å‡¦ç†
            combined = template.format(**cleaned_values)
    except KeyError as e:
        logger.warning(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨ã¦ã®å€¤ã‚’ç©ºç™½åŒºåˆ‡ã‚Šã§çµåˆ
        combined = " ".join(cleaned_values.values())

    return combined.strip()


def additional_preprocessing(df: pd.DataFrame, dataset_type: str = None) -> pd.DataFrame:
    """
    ãã®ä»–ã®å‰å‡¦ç†
    - é‡è¤‡è¡Œã®é™¤å»
    - ç©ºè¡Œã®é™¤å»
    - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚»ãƒƒãƒˆ

    Args:
        df (pd.DataFrame): å‰å‡¦ç†å¯¾è±¡ã®DataFrame
        dataset_type (str, optional): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—

    Returns:
        pd.DataFrame: å‰å‡¦ç†æ¸ˆã¿DataFrame
    """
    initial_rows = len(df)

    # é‡è¤‡è¡Œã®é™¤å»
    df = df.drop_duplicates()
    duplicates_removed = initial_rows - len(df)

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ã®å¿…é ˆåˆ—ã‚’å–å¾—
    if dataset_type:
        config_data = RAGConfig.get_config(dataset_type)
        required_columns = config_data["required_columns"]
        # å­˜åœ¨ã™ã‚‹å¿…é ˆåˆ—ã®ã¿ã§ãƒ•ã‚£ãƒ«ã‚¿
        existing_required = [col for col in required_columns if col in df.columns]
        if existing_required:
            df = df.dropna(subset=existing_required)
    else:
        # æ±ç”¨å‡¦ç†: å…¨åˆ—ãŒNAã®è¡Œã‚’é™¤å»
        df = df.dropna(how='all')

    empty_rows_removed = initial_rows - duplicates_removed - len(df)

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚»ãƒƒãƒˆ
    df = df.reset_index(drop=True)

    # å‡¦ç†çµ±è¨ˆã‚’ãƒ­ã‚°å‡ºåŠ›
    logger.info(f"å‰å‡¦ç†å®Œäº†: é‡è¤‡é™¤å»={duplicates_removed}è¡Œ, ç©ºè¡Œé™¤å»={empty_rows_removed}è¡Œ")

    return df


# ==================================================
# ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã®å…±é€šé–¢æ•°
# ==================================================
def validate_data(df: pd.DataFrame, dataset_type: str = None) -> List[str]:
    """
    ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼

    Args:
        df (pd.DataFrame): æ¤œè¨¼å¯¾è±¡ã®DataFrame
        dataset_type (str, optional): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—

    Returns:
        List[str]: æ¤œè¨¼çµæœãƒ»å•é¡Œç‚¹ã®ãƒªã‚¹ãƒˆ
    """
    issues = []

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã®å–å¾—
    if dataset_type:
        config_data = RAGConfig.get_config(dataset_type)
        required_columns = config_data["required_columns"]
    else:
        required_columns = []

    # åŸºæœ¬çµ±è¨ˆ
    issues.append(f"ç·è¡Œæ•°: {len(df)}")
    issues.append(f"ç·åˆ—æ•°: {len(df.columns)}")

    # å¿…é ˆåˆ—ã®å­˜åœ¨ç¢ºèª
    if required_columns:
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            issues.append(f"âš ï¸ å¿…é ˆåˆ—ãŒä¸è¶³: {missing_columns}")
        else:
            issues.append(f"âœ… å¿…é ˆåˆ—ç¢ºèªæ¸ˆã¿: {required_columns}")

    # å„åˆ—ã®ç©ºå€¤ç¢ºèª
    for col in df.columns:
        empty_count = df[col].isna().sum() + (df[col] == '').sum()
        if empty_count > 0:
            percentage = (empty_count / len(df)) * 100
            issues.append(f"{col}åˆ—: ç©ºå€¤ {empty_count}å€‹ ({percentage:.1f}%)")

    # æ–‡å­—æ•°ã®ç¢ºèªï¼ˆå¿…é ˆåˆ—ã®ã¿ï¼‰
    for col in required_columns:
        if col in df.columns:
            text_lengths = df[col].astype(str).str.len()
            avg_length = text_lengths.mean()
            max_length = text_lengths.max()
            min_length = text_lengths.min()
            issues.append(f"{col}åˆ—: å¹³å‡{avg_length:.1f}æ–‡å­— (æœ€å°{min_length}, æœ€å¤§{max_length})")

    # é‡è¤‡è¡Œã®ç¢ºèª
    duplicate_count = df.duplicated().sum()
    if duplicate_count > 0:
        issues.append(f"âš ï¸ é‡è¤‡è¡Œ: {duplicate_count}å€‹")
    else:
        issues.append("âœ… é‡è¤‡è¡Œãªã—")

    return issues


# ==================================================
# ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®å…±é€šé–¢æ•°
# ==================================================
def load_dataset(uploaded_file, dataset_type: str = None) -> Tuple[pd.DataFrame, List[str]]:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã¨åŸºæœ¬æ¤œè¨¼

    Args:
        uploaded_file: Streamlitã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«
        dataset_type (str, optional): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—

    Returns:
        Tuple[pd.DataFrame, List[str]]: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨æ¤œè¨¼çµæœ
    """
    try:
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        df = pd.read_csv(uploaded_file)

        # åŸºæœ¬æ¤œè¨¼
        validation_results = validate_data(df, dataset_type)

        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ, {len(df.columns)}åˆ—")
        return df, validation_results

    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        raise


def create_download_data(df: pd.DataFrame,
                         include_combined: bool = True,
                         dataset_type: str = None) -> Tuple[str, Optional[str]]:
    """
    ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ

    Args:
        df (pd.DataFrame): å‡¦ç†æ¸ˆã¿DataFrame
        include_combined (bool): çµåˆãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚ã‚‹ã‹
        dataset_type (str, optional): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—

    Returns:
        Tuple[str, Optional[str]]: CSVãƒ‡ãƒ¼ã‚¿ã€çµåˆãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    """
    try:
        logger.info(f"create_download_dataé–‹å§‹: è¡Œæ•°={len(df)}, include_combined={include_combined}")

        # CSVãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        logger.info("CSVãƒ‡ãƒ¼ã‚¿ä½œæˆé–‹å§‹")
        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8')
        csv_data = csv_buffer.getvalue()
        logger.info(f"CSVãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†: é•·ã•={len(csv_data)}")

        # çµåˆãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        text_data = None
        if include_combined and 'Combined_Text' in df.columns:
            logger.info("çµåˆãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆé–‹å§‹")
            text_data = df['Combined_Text'].to_string(index=False)
            logger.info(f"çµåˆãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†: é•·ã•={len(text_data)}")
        else:
            if include_combined:
                logger.warning("Combined_Textåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            else:
                logger.info("çµåˆãƒ†ã‚­ã‚¹ãƒˆä½œæˆã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ")

        logger.info("create_download_dataå®Œäº†")
        return csv_data, text_data

    except Exception as e:
        logger.error(f"create_download_data ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")
        import traceback
        logger.error(f"ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯: {traceback.format_exc()}")
        raise


# ==================================================
# çµ±è¨ˆæƒ…å ±è¡¨ç¤ºã®å…±é€šé–¢æ•°
# ==================================================
def display_statistics(df_original: pd.DataFrame,
                       df_processed: pd.DataFrame,
                       dataset_type: str = None) -> None:
    """
    å‡¦ç†å‰å¾Œã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º

    Args:
        df_original (pd.DataFrame): å…ƒã®DataFrame
        df_processed (pd.DataFrame): å‡¦ç†å¾Œã®DataFrame
        dataset_type (str, optional): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
    """
    st.subheader("ğŸ“Š çµ±è¨ˆæƒ…å ±")

    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("å…ƒã®è¡Œæ•°", len(df_original))
    with col2:
        st.metric("å‡¦ç†å¾Œã®è¡Œæ•°", len(df_processed))
    with col3:
        removed_rows = len(df_original) - len(df_processed)
        st.metric("é™¤å»ã•ã‚ŒãŸè¡Œæ•°", removed_rows)

    # çµåˆãƒ†ã‚­ã‚¹ãƒˆã®åˆ†æ
    if 'Combined_Text' in df_processed.columns:
        st.subheader("ğŸ“ çµåˆå¾Œãƒ†ã‚­ã‚¹ãƒˆåˆ†æ")
        text_lengths = df_processed['Combined_Text'].str.len()

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("å¹³å‡æ–‡å­—æ•°", f"{text_lengths.mean():.0f}")
        with col2:
            st.metric("æœ€å¤§æ–‡å­—æ•°", text_lengths.max())
        with col3:
            st.metric("æœ€å°æ–‡å­—æ•°", text_lengths.min())

        # æ–‡å­—æ•°åˆ†å¸ƒã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
        try:
            import pandas as pd  # æœ€åˆã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

            # æ–‡å­—æ•°ã‚’é©åˆ‡ãªç¯„å›²ã§ãƒ“ãƒ³åˆ†ã‘
            min_length = int(text_lengths.min())
            max_length = int(text_lengths.max())

            # ãƒ“ãƒ³æ•°ã‚’èª¿æ•´ï¼ˆæœ€å¤§20å€‹ï¼‰
            num_bins = min(20, max(5, (max_length - min_length) // 100))

            if num_bins >= 2 and max_length > min_length:
                # ç­‰é–“éš”ã§ãƒ“ãƒ³ã‚’ä½œæˆ
                bin_edges = pd.cut(text_lengths, bins=num_bins, duplicates='drop')
                bin_counts = bin_edges.value_counts().sort_index()

                # ãƒ“ãƒ³ã®ãƒ©ãƒ™ãƒ«ã‚’æ•°å€¤ã«å¤‰æ›
                bin_data = {}
                for interval, count in bin_counts.items():
                    # åŒºé–“ã®ä¸­å¤®å€¤ã‚’ãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä½¿ç”¨
                    mid_point = int((interval.left + interval.right) / 2)
                    bin_data[f"{mid_point}æ–‡å­—"] = count

                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦è¡¨ç¤º
                if bin_data:
                    chart_df = pd.DataFrame.from_dict(bin_data, orient='index', columns=['ä»¶æ•°'])
                    st.bar_chart(chart_df)
                else:
                    st.info("æ–‡å­—æ•°åˆ†å¸ƒãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            else:
                st.info("æ–‡å­—æ•°ã®ç¯„å›²ãŒç‹­ãã€ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“")

        except Exception as e:
            logger.warning(f"ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡å˜ãªçµ±è¨ˆè¡¨ç¤º
            try:
                st.write("**æ–‡å­—æ•°åˆ†å¸ƒï¼ˆç°¡æ˜“ç‰ˆï¼‰**")
                percentiles = text_lengths.quantile([0.25, 0.5, 0.75]).round(0)
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("25%ç‚¹", f"{percentiles[0.25]:.0f}æ–‡å­—")
                with col2:
                    st.metric("ä¸­å¤®å€¤", f"{percentiles[0.5]:.0f}æ–‡å­—")
                with col3:
                    st.metric("75%ç‚¹", f"{percentiles[0.75]:.0f}æ–‡å­—")
            except Exception as e2:
                logger.error(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¡¨ç¤ºã‚‚ã‚¨ãƒ©ãƒ¼: {e2}")
                st.info("æ–‡å­—æ•°åˆ†å¸ƒã®è¡¨ç¤ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")


# ==================================================
# UIè¨­å®šã®å…±é€šé–¢æ•°
# ==================================================
def setup_rag_page(dataset_type: str = "medical_qa") -> None:
    """
    RAGãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒšãƒ¼ã‚¸ã®å…±é€šè¨­å®š

    Args:
        dataset_type (str): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
    """
    config_data = RAGConfig.get_config(dataset_type)

    st.set_page_config(
        page_title=f"{config_data['name']}å‰å‡¦ç†",
        page_icon=config_data['icon'],
        layout="wide"
    )

    st.title(f"{config_data['icon']} {config_data['name']}å‰å‡¦ç†ã‚¢ãƒ—ãƒª")
    st.markdown("---")

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã®è¡¨ç¤º
    with st.sidebar.expander("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±", expanded=True):
        st.write(f"**åå‰**: {config_data['name']}")
        st.write(f"**èª¬æ˜**: {config_data['description']}")
        st.write(f"**å¿…é ˆåˆ—**: {', '.join(config_data['required_columns'])}")


def setup_sidebar_controls(dataset_type: str = "medical_qa") -> Tuple[bool, bool]:
    """
    ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®åˆ¶å¾¡ãƒ‘ãƒãƒ«è¨­å®š

    Args:
        dataset_type (str): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—

    Returns:
        Tuple[bool, bool]: (åˆ—çµåˆã‚ªãƒ—ã‚·ãƒ§ãƒ³, ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼è¡¨ç¤ºã‚ªãƒ—ã‚·ãƒ§ãƒ³)
    """
    st.sidebar.header("è¨­å®š")

    combine_columns_option = st.sidebar.checkbox(
        "è¤‡æ•°åˆ—ã‚’çµåˆã™ã‚‹ï¼ˆVector Storeç”¨ï¼‰",
        value=True,
        help="è¤‡æ•°åˆ—ã‚’çµåˆã—ã¦RAGç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ"
    )

    show_validation = st.sidebar.checkbox(
        "ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚’è¡¨ç¤º",
        value=True,
        help="ãƒ‡ãƒ¼ã‚¿ã®å“è³ªæ¤œè¨¼çµæœã‚’è¡¨ç¤º"
    )

    return combine_columns_option, show_validation


# ==================================================
# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãã®å‡¦ç†é–¢æ•°
# ==================================================
# å…ƒã®é–¢æ•°ï¼ˆã‚¹ãƒ”ãƒŠãƒ¼ã‚ã‚Šï¼‰:
# @error_handler_ui
# def process_rag_data(df: pd.DataFrame,
#                      dataset_type: str,
#                      combine_columns_option: bool = True) -> pd.DataFrame:
@error_handler_ui
def process_rag_data(df: pd.DataFrame,
                     dataset_type: str,
                     combine_columns_option: bool = True) -> pd.DataFrame:
    """
    RAGãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’å®Ÿè¡Œ

    Args:
        df (pd.DataFrame): å…ƒã®DataFrame
        dataset_type (str): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        combine_columns_option (bool): åˆ—çµåˆã‚ªãƒ—ã‚·ãƒ§ãƒ³

    Returns:
        pd.DataFrame: å‡¦ç†æ¸ˆã¿DataFrame
    """
    # with st.spinner("å‰å‡¦ç†ä¸­..."):  # â† ã“ã®è¡Œã‚’å‰Šé™¤
    # åŸºæœ¬çš„ãªå‰å‡¦ç†
    df_processed = additional_preprocessing(df.copy(), dataset_type)

    # å„åˆ—ã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    config_data = RAGConfig.get_config(dataset_type)
    required_columns = config_data["required_columns"]

    for col in required_columns:
        if col in df_processed.columns:
            df_processed[col] = df_processed[col].apply(clean_text)

    # åˆ—ã®çµåˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if combine_columns_option:
        df_processed['Combined_Text'] = df_processed.apply(
            lambda row: combine_columns(row, dataset_type),
            axis=1
        )

    return df_processed


# ==================================================
# ä½¿ç”¨æ–¹æ³•èª¬æ˜ã®å…±é€šé–¢æ•°
# ==================================================
def show_usage_instructions(dataset_type: str = "medical_qa") -> None:
    """
    ä½¿ç”¨æ–¹æ³•ã®èª¬æ˜ã‚’è¡¨ç¤º

    Args:
        dataset_type (str): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
    """
    config_data = RAGConfig.get_config(dataset_type)
    required_columns_str = ", ".join(config_data["required_columns"])

    st.markdown("---")
    st.subheader("ğŸ“– ä½¿ç”¨æ–¹æ³•")
    st.markdown(f"""
    1. **CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: {required_columns_str} ã®åˆ—ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
    2. **å‰å‡¦ç†ã‚’å®Ÿè¡Œ**: ä»¥ä¸‹ã®å‡¦ç†ãŒè‡ªå‹•ã§å®Ÿè¡Œã•ã‚Œã¾ã™ï¼š
       - æ”¹è¡Œã®é™¤å»
       - é€£ç¶šã—ãŸç©ºç™½ã®çµ±ä¸€
       - é‡è¤‡è¡Œã®é™¤å»
       - ç©ºè¡Œã®é™¤å»
       - å¼•ç”¨ç¬¦ã®æ­£è¦åŒ–
    3. **è¤‡æ•°åˆ—çµåˆ**: Vector Store/RAGç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸè‡ªç„¶ãªæ–‡ç« ã¨ã—ã¦çµåˆ
    4. **ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

    **Vector Storeç”¨æœ€é©åŒ–:**
    - è‡ªç„¶ãªæ–‡ç« ã¨ã—ã¦çµåˆï¼ˆãƒ©ãƒ™ãƒ«æ–‡å­—åˆ—ãªã—ï¼‰
    - OpenAI embeddingãƒ¢ãƒ‡ãƒ«ã«æœ€é©åŒ–
    - æ¤œç´¢æ€§èƒ½ãŒå‘ä¸Š

    **è¿½åŠ ã®å‰å‡¦ç†é …ç›®:**
    - é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®é™¤å»
    - ç©ºå€¤ã®å‡¦ç†
    - æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®çµ±ä¸€
    - ç‰¹æ®Šæ–‡å­—ã®æ­£è¦åŒ–
    - ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã¨ãƒ¬ãƒãƒ¼ãƒˆ
    """)


# ==================================================
# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
# ==================================================
__all__ = [
    # è¨­å®šã‚¯ãƒ©ã‚¹
    'RAGConfig',

    # å‰å‡¦ç†é–¢æ•°
    'clean_text',
    'combine_columns',
    'additional_preprocessing',

    # æ¤œè¨¼ãƒ»ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
    'validate_data',
    'load_dataset',
    'create_download_data',

    # UIé–¢é€£
    'setup_rag_page',
    'setup_sidebar_controls',
    'display_statistics',
    'show_usage_instructions',

    # å‡¦ç†é–¢æ•°
    'process_rag_data',
]
