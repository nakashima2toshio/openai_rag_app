# a30_011_make_rag_data_customer.py
# ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿ã®RAGå‰å‡¦ç†ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
# streamlit run a30_011_make_rag_data_customer.py --server.port=8501

import streamlit as st
import pandas as pd
from typing import List, Dict, Any
from pathlib import Path
from datetime import datetime

# å…±é€šRAGãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from helper_rag import (
        # è¨­å®šãƒ»å®šæ•°
        RAGConfig,

        # å‰å‡¦ç†é–¢æ•°ï¼ˆç§»è¡Œæ¸ˆã¿ï¼‰
        clean_text,
        combine_columns,
        additional_preprocessing,
        validate_data,

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        load_dataset,
        create_download_data,
        process_rag_data,

        # UIé–¢é€£
        setup_rag_page,
        setup_sidebar_controls,
        display_statistics,
        show_usage_instructions,
    )
except ImportError as e:
    st.error(f"helper_rag.pyã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    st.info("helper_rag.pyãŒåŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# æ—¢å­˜ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚‚ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from helper_st import UIHelper, error_handler_ui
    from helper_api import logger
except ImportError as e:
    st.warning(f"æ—¢å­˜ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ä¸€éƒ¨ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}")
    # åŸºæœ¬çš„ãªãƒ­ã‚°è¨­å®š
    import logging

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)


# ==================================================
# ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–¢æ•°
# ==================================================
def create_output_directory() -> Path:
    """OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ"""
    try:
        import os
        current_dir = os.getcwd()

        # ç›¸å¯¾ãƒ‘ã‚¹ã§OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        output_dir = Path("OUTPUT")
        absolute_output_dir = output_dir.resolve()

        logger.info(f"ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {current_dir}")
        logger.info(f"OUTPUTç›¸å¯¾ãƒ‘ã‚¹: {output_dir}")
        logger.info(f"OUTPUTçµ¶å¯¾ãƒ‘ã‚¹: {absolute_output_dir}")

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        output_dir.mkdir(exist_ok=True)

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå®Ÿéš›ã«ä½œæˆã•ã‚ŒãŸã‹ç¢ºèª
        if not output_dir.exists():
            raise OSError(f"OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã«å¤±æ•—: {absolute_output_dir}")

        # æ›¸ãè¾¼ã¿æ¨©é™ã®ãƒ†ã‚¹ãƒˆ
        test_file = output_dir / ".test_write"
        try:
            test_file.write_text("test", encoding='utf-8')
            if test_file.exists():
                test_file.unlink()
                logger.info("æ›¸ãè¾¼ã¿æ¨©é™ãƒ†ã‚¹ãƒˆ: æˆåŠŸ")
            else:
                raise PermissionError("ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—")
        except Exception as e:
            raise PermissionError(f"æ›¸ãè¾¼ã¿æ¨©é™ãƒ†ã‚¹ãƒˆã«å¤±æ•—: {e}")

        logger.info(f"OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™å®Œäº†: {absolute_output_dir}")
        return output_dir

    except PermissionError as e:
        logger.error(f"æ¨©é™ã‚¨ãƒ©ãƒ¼: {e}")
        raise PermissionError(f"OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆ/æ›¸ãè¾¼ã¿æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“: {e}")
    except Exception as e:
        logger.error(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        raise Exception(f"OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


def save_files_to_output(df_processed, dataset_type: str, csv_data: str, text_data: str = None) -> Dict[str, str]:
    """
    å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜

    Args:
        df_processed: å‡¦ç†æ¸ˆã¿DataFrame
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        csv_data: CSVãƒ‡ãƒ¼ã‚¿æ–‡å­—åˆ—
        text_data: çµåˆãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

    Returns:
        Dict[str, str]: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¾æ›¸
    """
    try:
        # ãƒ‡ãƒãƒƒã‚°: ç¾åœ¨ã®ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèª
        import os
        current_dir = os.getcwd()
        logger.info(f"ç¾åœ¨ã®ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {current_dir}")

        # OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        output_dir = create_output_directory()
        logger.info(f"OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹: {output_dir.absolute()}")

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        saved_files = {}

        # ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
        if not csv_data or len(csv_data.strip()) == 0:
            raise ValueError("CSVãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")

        logger.info(f"ä¿å­˜é–‹å§‹: {dataset_type}, {len(df_processed)}è¡Œ, ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {timestamp}")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
        csv_filename = f"preprocessed_{dataset_type}_{len(df_processed)}rows_{timestamp}.csv"
        csv_path = output_dir / csv_filename
        logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–‹å§‹: {csv_path}")

        with open(csv_path, 'w', encoding='utf-8') as f:
            f.write(csv_data)

        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå®Ÿéš›ã«ä½œæˆã•ã‚ŒãŸã‹ç¢ºèª
        if csv_path.exists():
            file_size = csv_path.stat().st_size
            saved_files['csv'] = str(csv_path)
            logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {csv_path} ({file_size} bytes)")
        else:
            raise IOError(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—: {csv_path}")

        # çµåˆãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
        if text_data and len(text_data.strip()) > 0:
            # txt_filename = f"combined_{dataset_type}_{len(df_processed)}rows_{timestamp}.txt"
            txt_filename = f"{dataset_type}.txt"  # å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«åã«å¤‰æ›´
            txt_path = output_dir / txt_filename
            logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–‹å§‹: {txt_path}")

            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(text_data)

            if txt_path.exists():
                file_size = txt_path.stat().st_size
                saved_files['txt'] = str(txt_path)
                logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {txt_path} ({file_size} bytes)")
            else:
                logger.warning(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—: {txt_path}")
        else:
            logger.info("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—")

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
        metadata = {
            "dataset_type"        : dataset_type,
            "original_rows"       : st.session_state.get('original_rows', 0),
            "processed_rows"      : len(df_processed),
            "processing_timestamp": timestamp,
            "created_at"          : datetime.now().isoformat(),
            "working_directory"   : current_dir,
            "output_directory"    : str(output_dir.absolute()),
            "files_created"       : list(saved_files.keys())
        }

        metadata_filename = f"metadata_{dataset_type}_{timestamp}.json"
        metadata_path = output_dir / metadata_filename
        logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–‹å§‹: {metadata_path}")

        # helper_api.pyã®safe_json_dumpsã‚’ä½¿ç”¨
        try:
            from helper_api import safe_json_dumps
            metadata_json = safe_json_dumps(metadata)
        except ImportError:
            import json
            metadata_json = json.dumps(metadata, indent=2, ensure_ascii=False)

        with open(metadata_path, 'w', encoding='utf-8') as f:
            f.write(metadata_json)

        if metadata_path.exists():
            file_size = metadata_path.stat().st_size
            saved_files['metadata'] = str(metadata_path)
            logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {metadata_path} ({file_size} bytes)")
        else:
            logger.warning(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—: {metadata_path}")

        logger.info(f"å…¨ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {len(saved_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜")

        # æœ€çµ‚ç¢ºèª: ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        for file_type, file_path in saved_files.items():
            if not Path(file_path).exists():
                logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_type} - {file_path}")

        return saved_files

    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")
        import traceback
        logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")
        raise


# ==================================================
# ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQç‰¹æœ‰ã®å‡¦ç†é–¢æ•°
# ==================================================
def validate_customer_support_data_specific(df) -> List[str]:
    """
    ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®æ¤œè¨¼

    Args:
        df: æ¤œè¨¼å¯¾è±¡ã®DataFrame

    Returns:
        List[str]: ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®æ¤œè¨¼çµæœ
    """
    support_issues = []

    # ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆé–¢é€£ç”¨èªã®å­˜åœ¨ç¢ºèª
    support_keywords = [
        'å•é¡Œ', 'è§£æ±º', 'ãƒˆãƒ©ãƒ–ãƒ«', 'ã‚¨ãƒ©ãƒ¼', 'ã‚µãƒãƒ¼ãƒˆ', 'ãƒ˜ãƒ«ãƒ—', 'å¯¾å¿œ',
        'problem', 'issue', 'error', 'help', 'support', 'solution', 'troubleshoot'
    ]

    if 'question' in df.columns:
        questions_with_support_terms = 0
        for _, row in df.iterrows():
            question_text = str(row.get('question', '')).lower()
            if any(keyword in question_text for keyword in support_keywords):
                questions_with_support_terms += 1

        support_ratio = (questions_with_support_terms / len(df)) * 100
        support_issues.append(f"ã‚µãƒãƒ¼ãƒˆé–¢é€£ç”¨èªã‚’å«ã‚€è³ªå•: {questions_with_support_terms}ä»¶ ({support_ratio:.1f}%)")

    # å›ç­”ã®é•·ã•åˆ†æï¼ˆã‚µãƒãƒ¼ãƒˆå›ç­”ã¯å®Ÿç”¨çš„ãªé•·ã•ï¼‰
    if 'answer' in df.columns:
        answer_lengths = df['answer'].astype(str).str.len()
        avg_answer_length = answer_lengths.mean()
        if avg_answer_length < 50:
            support_issues.append(f"âš ï¸ å¹³å‡å›ç­”é•·ãŒçŸ­ã„å¯èƒ½æ€§: {avg_answer_length:.0f}æ–‡å­—")
        else:
            support_issues.append(f"âœ… é©åˆ‡ãªå›ç­”é•·: å¹³å‡{avg_answer_length:.0f}æ–‡å­—")

    # è³ªå•ã®ç¨®é¡åˆ†æï¼ˆç°¡æ˜“ç‰ˆï¼‰
    if 'question' in df.columns:
        question_starters = ['ã©ã†ã™ã‚Œã°', 'ãªãœ', 'ã„ã¤', 'ã©ã“ã§', 'ã©ã®ã‚ˆã†ã«', 'what', 'how', 'why', 'when',
                             'where']
        question_type_count = 0
        for _, row in df.iterrows():
            question_text = str(row.get('question', '')).lower()
            if any(starter in question_text for starter in question_starters):
                question_type_count += 1

        question_type_ratio = (question_type_count / len(df)) * 100
        support_issues.append(f"ç–‘å•å½¢è³ªå•: {question_type_count}ä»¶ ({question_type_ratio:.1f}%)")

    return support_issues


# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•°
# ==================================================
@error_handler_ui
def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•°"""

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã®è¨­å®š
    DATASET_TYPE = "customer_support_faq"

    # ãƒšãƒ¼ã‚¸è¨­å®šï¼ˆç‹¬ç«‹å®Ÿè¡Œï¼‰
    try:
        st.set_page_config(
            page_title="ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†",
            page_icon="ğŸ’¬",
            layout="wide"
        )
    except st.errors.StreamlitAPIException:
        # æ—¢ã«è¨­å®šæ¸ˆã¿ã®å ´åˆã¯ç„¡è¦–
        pass

    st.title("ğŸ’¬ ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚¢ãƒ—ãƒª")
    st.markdown("---")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®šï¼ˆç‹¬ç«‹å®Ÿè¡Œï¼‰
    st.sidebar.header("è¨­å®š")
    combine_columns_option = st.sidebar.checkbox(
        "è¤‡æ•°åˆ—ã‚’çµåˆã™ã‚‹ï¼ˆVector Storeç”¨ï¼‰",
        value=True,
        help="è¤‡æ•°åˆ—ã‚’çµåˆã—ã¦RAGç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ"
    )
    show_validation = st.sidebar.checkbox(
        "ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚’è¡¨ç¤º",
        value=True,
        help="ãƒ‡ãƒ¼ã‚¿ã®å“è³ªæ¤œè¨¼çµæœã‚’è¡¨ç¤º"
    )

    # ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®è¨­å®š
    with st.sidebar.expander("ğŸ’¬ ã‚µãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿è¨­å®š", expanded=False):
        preserve_formatting = st.checkbox(
            "æ›¸å¼ã‚’ä¿è­·",
            value=True,
            help="å›ç­”å†…ã®é‡è¦ãªæ›¸å¼ï¼ˆç•ªå·ä»˜ããƒªã‚¹ãƒˆãªã©ï¼‰ã‚’ä¿è­·"
        )
        normalize_questions = st.checkbox(
            "è³ªå•ã‚’æ­£è¦åŒ–",
            value=True,
            help="è³ªå•æ–‡ã®è¡¨è¨˜ã‚†ã‚Œã‚’çµ±ä¸€"
        )

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    st.subheader("ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_file = st.file_uploader(
        "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
        type=['csv'],
        help="question, answer ã®2åˆ—ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«"
    )

    if uploaded_file is not None:
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®ç¢ºèª
            st.info(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {uploaded_file.name} ({uploaded_file.size:,} bytes)")

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†çŠ¶æ³ã‚’ç®¡ç†
            file_key = f"file_{uploaded_file.name}_{uploaded_file.size}"

            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã¯å†èª­ã¿è¾¼ã¿
            if st.session_state.get('current_file_key') != file_key:
                # DataFrameã®èª­ã¿è¾¼ã¿ï¼ˆload_dataseté–¢æ•°ã‚’ä½¿ç”¨ï¼‰
                with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                    df, validation_results = load_dataset(uploaded_file, DATASET_TYPE)

                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                st.session_state['current_file_key'] = file_key
                st.session_state['original_df'] = df
                st.session_state['validation_results'] = validation_results
                st.session_state['original_rows'] = len(df)
                st.session_state['file_processed'] = False

                logger.info(f"æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿: {len(df)}è¡Œ")
            else:
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰å–å¾—
                df = st.session_state['original_df']
                validation_results = st.session_state['validation_results']
                logger.info(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—: {len(df)}è¡Œ")

            st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸã€‚è¡Œæ•°: {len(df)}")

            # å…ƒãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
            st.subheader("ğŸ“‹ å…ƒãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            st.dataframe(df.head(10))

            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼çµæœã®è¡¨ç¤º
            if show_validation:
                st.subheader("ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼")

                # åŸºæœ¬æ¤œè¨¼çµæœ
                for issue in validation_results:
                    st.info(issue)

                # ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®æ¤œè¨¼
                support_issues = validate_customer_support_data_specific(df)
                if support_issues:
                    st.write("**ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®åˆ†æ:**")
                    for issue in support_issues:
                        st.info(issue)

            # å‰å‡¦ç†å®Ÿè¡Œ
            st.subheader("âš™ï¸ å‰å‡¦ç†å®Ÿè¡Œ")

            if st.button("å‰å‡¦ç†ã‚’å®Ÿè¡Œ", type="primary", key="process_button"):
                try:
                    with st.spinner("å‰å‡¦ç†ä¸­..."):
                        # RAGãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
                        df_processed = process_rag_data(
                            df.copy(),  # ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆã—ã¦å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿è­·
                            DATASET_TYPE,
                            combine_columns_option
                        )

                    st.success("å‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                    st.session_state['processed_df'] = df_processed
                    st.session_state['file_processed'] = True

                    # å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
                    st.subheader("âœ… å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                    st.dataframe(df_processed.head(10))

                    # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
                    display_statistics(df, df_processed, DATASET_TYPE)

                    # ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®å¾Œå‡¦ç†åˆ†æ
                    if 'Combined_Text' in df_processed.columns:
                        st.subheader("ğŸ’¬ ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®åˆ†æ")

                        # çµåˆãƒ†ã‚­ã‚¹ãƒˆã®ã‚µãƒãƒ¼ãƒˆç”¨èªåˆ†æ
                        combined_texts = df_processed['Combined_Text']
                        support_keywords = ['å•é¡Œ', 'ã‚¨ãƒ©ãƒ¼', 'ãƒˆãƒ©ãƒ–ãƒ«', 'ã‚µãƒãƒ¼ãƒˆ', 'ãƒ˜ãƒ«ãƒ—']

                        keyword_counts = {}
                        for keyword in support_keywords:
                            count = combined_texts.str.contains(keyword, case=False).sum()
                            keyword_counts[keyword] = count

                        if keyword_counts:
                            st.write("**ã‚µãƒãƒ¼ãƒˆé–¢é€£ç”¨èªã®å‡ºç¾é »åº¦:**")
                            for keyword, count in keyword_counts.items():
                                percentage = (count / len(df_processed)) * 100
                                st.write(f"- {keyword}: {count}ä»¶ ({percentage:.1f}%)")

                        # è³ªå•ã®é•·ã•åˆ†å¸ƒ
                        if 'question' in df_processed.columns:
                            question_lengths = df_processed['question'].str.len()
                            st.write("**è³ªå•ã®é•·ã•çµ±è¨ˆ:**")
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("å¹³å‡è³ªå•é•·", f"{question_lengths.mean():.0f}æ–‡å­—")
                            with col2:
                                st.metric("æœ€é•·è³ªå•", f"{question_lengths.max()}æ–‡å­—")
                            with col3:
                                st.metric("æœ€çŸ­è³ªå•", f"{question_lengths.min()}æ–‡å­—")

                    logger.info(f"ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†: {len(df)} â†’ {len(df_processed)}è¡Œ")

                except Exception as process_error:
                    st.error(f"å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(process_error)}")
                    logger.error(f"å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {process_error}")

                    with st.expander("ğŸ”§ å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼è©³ç´°", expanded=False):
                        import traceback
                        st.code(traceback.format_exc())

            # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ä¿å­˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¤º
            if st.session_state.get('file_processed', False) and 'processed_df' in st.session_state:
                df_processed = st.session_state['processed_df']

                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ä¿å­˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³
                st.subheader("ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ä¿å­˜")

                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
                if 'download_data' not in st.session_state or st.session_state.get('download_data_key') != file_key:
                    with st.spinner("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­..."):
                        csv_data, text_data = create_download_data(
                            df_processed,
                            combine_columns_option,
                            DATASET_TYPE
                        )
                        st.session_state['download_data'] = (csv_data, text_data)
                        st.session_state['download_data_key'] = file_key
                else:
                    csv_data, text_data = st.session_state['download_data']

                # ãƒ–ãƒ©ã‚¦ã‚¶ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                st.write("**ğŸ“¥ ãƒ–ãƒ©ã‚¦ã‚¶ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**")
                col1, col2 = st.columns(2)

                with col1:
                    st.download_button(
                        label="ğŸ“Š CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=csv_data,
                        file_name=f"preprocessed_{DATASET_TYPE}_{len(df_processed)}rows.csv",
                        mime="text/csv",
                        help="å‰å‡¦ç†æ¸ˆã¿ã®ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿ã‚’CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
                    )

                with col2:
                    if text_data:
                        st.download_button(
                            label="ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=text_data,
                            # file_name=f"combined_{DATASET_TYPE}_{len(df_processed)}rows.txt",
                            file_name=f"customer_support_faq.txt",
                            mime="text/plain",
                            help="Vector Store/RAGç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸçµåˆãƒ†ã‚­ã‚¹ãƒˆ"
                        )

                # ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜
                st.write("**ğŸ’¾ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼ˆOUTPUTãƒ•ã‚©ãƒ«ãƒ€ï¼‰**")

                if st.button("ğŸ”„ OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜", type="secondary", key="save_button"):
                    try:
                        with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ä¸­..."):
                            logger.info("=== ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å‡¦ç†é–‹å§‹ ===")
                            saved_files = save_files_to_output(
                                df_processed,
                                DATASET_TYPE,
                                csv_data,
                                text_data
                            )
                            logger.info(f"=== ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å‡¦ç†å®Œäº†: {saved_files} ===")

                        if saved_files:
                            st.success("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†ï¼")

                            # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
                            with st.expander("ğŸ“‚ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§", expanded=True):
                                for file_type, file_path in saved_files.items():
                                    if Path(file_path).exists():
                                        file_size = Path(file_path).stat().st_size
                                        st.write(f"**{file_type.upper()}**: `{file_path}` ({file_size:,} bytes) âœ…")
                                    else:
                                        st.write(
                                            f"**{file_type.upper()}**: `{file_path}` âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

                                # OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã®å ´æ‰€ã‚’è¡¨ç¤º
                                output_path = Path("OUTPUT").resolve()
                                st.write(f"**ä¿å­˜å ´æ‰€**: `{output_path}`")
                                file_count = len(list(output_path.glob("*")))
                                st.write(f"**ãƒ•ã‚©ãƒ«ãƒ€å†…ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {file_count}å€‹")
                        else:
                            st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")

                    except Exception as save_error:
                        st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(save_error)}")
                        logger.error(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {save_error}")

                        with st.expander("ğŸ”§ ä¿å­˜ã‚¨ãƒ©ãƒ¼è©³ç´°", expanded=False):
                            import traceback
                            st.code(traceback.format_exc())

                # æ—¢å­˜ã®ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
                try:
                    output_dir = Path("OUTPUT")
                    if output_dir.exists():
                        saved_files_list = list(output_dir.glob(f"*{DATASET_TYPE}*"))
                        if saved_files_list:
                            st.write("**ğŸ“ æ—¢å­˜ã®ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«**")
                            with st.expander("ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§", expanded=False):
                                for file_path in sorted(saved_files_list, reverse=True)[:5]:  # æœ€æ–°5ä»¶
                                    file_stats = file_path.stat()
                                    file_time = datetime.fromtimestamp(file_stats.st_mtime).strftime(
                                        "%Y-%m-%d %H:%M:%S")
                                    st.write(f"- `{file_path.name}` ({file_stats.st_size:,} bytes, {file_time})")
                except Exception as list_error:
                    logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {list_error}")

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

            # è©³ç´°ãªã‚¨ãƒ©ãƒ¼è¨ºæ–­
            with st.expander("ğŸ”§ è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±", expanded=True):
                import traceback
                st.code(traceback.format_exc())

                # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®è©³ç´°ç¢ºèª
                st.write("**ãƒ•ã‚¡ã‚¤ãƒ«è¨ºæ–­:**")
                if uploaded_file is not None:
                    st.write(f"- ãƒ•ã‚¡ã‚¤ãƒ«å: {uploaded_file.name}")
                    st.write(f"- ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {uploaded_file.size:,} bytes")
                    st.write(f"- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: {uploaded_file.type}")
                    st.write(f"- ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå‹: {type(uploaded_file)}")

                    # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®ãƒ†ã‚¹ãƒˆèª­ã¿è¾¼ã¿
                    try:
                        uploaded_file.seek(0)  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ
                        sample_content = uploaded_file.read(200).decode('utf-8', errors='ignore')
                        st.write("**ãƒ•ã‚¡ã‚¤ãƒ«å…ˆé ­ï¼ˆ200æ–‡å­—ï¼‰:**")
                        st.code(sample_content)
                        uploaded_file.seek(0)  # ãƒªã‚»ãƒƒãƒˆ
                    except Exception as read_error:
                        st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {read_error}")
                else:
                    st.write("- ãƒ•ã‚¡ã‚¤ãƒ«: None")

                # ç’°å¢ƒæƒ…å ±
                st.write("**ç’°å¢ƒæƒ…å ±:**")
                st.write(f"- pandas version: {pd.__version__}")
                st.write(f"- streamlit version: {st.__version__}")

    else:
        st.info("ğŸ‘† CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")

    # ä½¿ç”¨æ–¹æ³•ã®èª¬æ˜
    show_usage_instructions(DATASET_TYPE)

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    if st.sidebar.checkbox("ğŸ”§ ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’è¡¨ç¤º", value=False):
        with st.sidebar.expander("ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹", expanded=False):
            # åŸºæœ¬æƒ…å ±
            st.write(f"**ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚­ãƒ¼**: {st.session_state.get('current_file_key', 'None')}")
            st.write(f"**ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ¸ˆã¿**: {st.session_state.get('file_processed', False)}")

            if 'original_df' in st.session_state:
                df = st.session_state['original_df']
                st.write(f"**å…ƒãƒ‡ãƒ¼ã‚¿**: {len(df)}è¡Œ")

            if 'processed_df' in st.session_state:
                df_processed = st.session_state['processed_df']
                st.write(f"**å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿**: {len(df_processed)}è¡Œ")

            # OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã®çŠ¶æ…‹
            try:
                output_dir = Path("OUTPUT")
                if output_dir.exists():
                    file_count = len(list(output_dir.glob(f"*{DATASET_TYPE}*")))
                    st.write(f"**ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«**: {file_count}å€‹")
                    st.write(f"**ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹**: `{output_dir.resolve()}`")
                else:
                    st.write("**OUTPUTãƒ•ã‚©ãƒ«ãƒ€**: æœªä½œæˆ")
            except Exception as e:
                st.write(f"**ãƒ•ã‚©ãƒ«ãƒ€çŠ¶æ…‹**: ã‚¨ãƒ©ãƒ¼ ({e})")


# ==================================================
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
# ==================================================
if __name__ == "__main__":
    main()

# å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰:
# streamlit run a30_011_make_rag_data_customer.py --server.port=8501
